# Penn Treebank Language Modeling Configuration - Overfitting Fix
# Optimized to reduce overfitting and improve generalization

# Model Configuration
model:
  type: "LSTM"  
  vocab_size: 18057   # Updated to recommended vocabulary size from analysis
  embedding_dim: 400  # Reduced from 650 to limit model capacity
  hidden_dim: 400     # Reduced from 650 to limit model capacity
  num_layers: 2       # Standard for PTB LSTM
  dropout: 0.65       # Increased from 0.5 for stronger regularization
  tie_weights: true

# Training Configuration
training:
  batch_size: 64      # Keep same batch size
  sequence_length: 38 # Keep same sequence length
  learning_rate: 0.0005 # Reduced from 0.001 for more stable training
  max_epochs: 40      # Keep same max epochs
  gradient_clip: 0.25 # Keep same gradient clipping
  patience: 3         # Reduced from 5 for earlier stopping
  warmup_steps: 500   # Reduced warmup for faster convergence
  
# Data Configuration
data:
  data_dir: "data/ptb" # Relative path for Colab environment
  train_file: "ptb.train.txt"
  valid_file: "ptb.valid.txt"
  test_file: "ptb.test.txt"
  min_freq: 3          # Keep same vocabulary settings
  max_vocab_size: 20000
  
# Logging and Checkpoints
logging:
  log_interval: 100   
  save_dir: "checkpoints_overfitting_fix"
  tensorboard_dir: "runs_overfitting_fix"
  save_best_only: true
  
# Evaluation
evaluation:
  eval_batch_size: 64 
  eval_interval: 1    # Evaluate every epoch for close monitoring
  
# Advanced Training Options
advanced:
  use_scheduler: true
  scheduler_type: "reduce_on_plateau" # Better for overfitting scenarios
  scheduler_patience: 2               # Reduce LR after 2 epochs without improvement
  scheduler_factor: 0.5               # Reduce LR by half
  weight_decay: 1e-4                  # Significantly increased from 1e-6
  label_smoothing: 0.1                # Add label smoothing for regularization
