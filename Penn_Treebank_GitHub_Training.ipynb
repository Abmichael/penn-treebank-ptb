{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7475a970",
   "metadata": {},
   "source": [
    "# Penn Treebank Language Modeling\n",
    "*A complete implementation of LSTM-based language modeling on the Penn Treebank dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d23b7e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates end-to-end language modeling on the Penn Treebank dataset with the following features:\n",
    "\n",
    "**üéØ Key Features:**\n",
    "- **GitHub Integration**: Project loaded directly from repository\n",
    "- **Zero Setup**: No manual uploads required\n",
    "- **GPU Acceleration**: Optimized for Colab GPU training\n",
    "- **Interactive Evaluation**: Generate text with custom seed words\n",
    "- **Comprehensive Analysis**: Data exploration and model evaluation\n",
    "\n",
    "**üìã What You'll Learn:**\n",
    "- Penn Treebank dataset characteristics and preprocessing\n",
    "- LSTM architecture for language modeling\n",
    "- Training strategies and hyperparameter tuning\n",
    "- Model evaluation and text generation\n",
    "- Interactive text generation with custom prompts\n",
    "\n",
    "**‚öôÔ∏è Requirements:**\n",
    "- Google Colab with GPU runtime\n",
    "- Public GitHub repository (automatically cloned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9078bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update with your GitHub repository\n",
    "GITHUB_REPO = \"https://github.com/Abmichael/penn-treebank-ptb.git\"\n",
    "PROJECT_NAME = \"penn-treebank-ptb\"\n",
    "\n",
    "print(\"üöÄ Penn Treebank Language Modeling\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5317d02",
   "metadata": {},
   "source": [
    "## 1. Project Setup\n",
    "\n",
    "First, we'll clone the project from GitHub and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e489e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Clone project from GitHub\n",
    "if not os.path.exists(PROJECT_NAME):\n",
    "    print(\"üì¶ Cloning project from GitHub...\")\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'clone', GITHUB_REPO], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(\"‚úÖ Project cloned successfully!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to clone repository: {e}\")\n",
    "        print(\"Please ensure the repository URL is correct and publicly accessible\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    print(\"‚úÖ Project already exists\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(PROJECT_NAME)\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Create local directories for generated files\n",
    "local_dirs = ['checkpoints', 'runs', 'logs', 'outputs']\n",
    "for dir_name in local_dirs:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "print(\"\\nüìÇ Storage Strategy:\")\n",
    "print(\"   ‚Ä¢ Source code: GitHub repository\")\n",
    "print(\"   ‚Ä¢ Generated files: Local session storage\")\n",
    "print(\"   ‚Ä¢ Results: Available for download\")\n",
    "\n",
    "# Verify project structure\n",
    "required_items = ['src', 'config', 'data', 'requirements.txt']\n",
    "missing_items = [item for item in required_items if not os.path.exists(item)]\n",
    "\n",
    "if missing_items:\n",
    "    print(f\"‚ö†Ô∏è Missing required items: {missing_items}\")\n",
    "else:\n",
    "    print(\"‚úÖ Project structure verified!\")\n",
    "    print(f\"üìã Contents: {', '.join(os.listdir('.'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e44b1",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "Install dependencies and verify GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94578ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Import required libraries and check GPU\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "print(\"\\nüîç Environment Information:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "    print(\"‚úÖ GPU is ready for training!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not detected. Using CPU (will be slower)\")\n",
    "    print(\"üí° Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703c883",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Extract and process the Penn Treebank dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if data needs extraction\n",
    "data_dir = Path('data/ptb')\n",
    "archive_path = data_dir / 'LDC99T42_Penn_Treebank_3.tar.zst'\n",
    "processed_files = ['ptb.train.txt', 'ptb.valid.txt', 'ptb.test.txt']\n",
    "\n",
    "# Check if processed files exist\n",
    "all_processed_exist = all((data_dir / f).exists() for f in processed_files)\n",
    "\n",
    "if all_processed_exist:\n",
    "    print(\"‚úÖ Penn Treebank data already processed\")\n",
    "    for f in processed_files:\n",
    "        file_path = data_dir / f\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   üìÑ {f}: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(\"üîÑ Processing Penn Treebank data...\")\n",
    "    \n",
    "    # Install zstd if needed\n",
    "    try:\n",
    "        subprocess.run(['zstd', '--version'], capture_output=True, check=True)\n",
    "    except:\n",
    "        print(\"üì• Installing zstd...\")\n",
    "        !apt-get update -qq && apt-get install -y zstd\n",
    "    \n",
    "    # Extract archive\n",
    "    if archive_path.exists():\n",
    "        print(f\"üì¶ Extracting {archive_path.name}...\")\n",
    "        \n",
    "        # Decompress and extract\n",
    "        tar_path = archive_path.with_suffix('')\n",
    "        subprocess.run(['zstd', '-d', str(archive_path), '-o', str(tar_path)], check=True)\n",
    "        \n",
    "        with tarfile.open(tar_path, 'r') as tar:\n",
    "            tar.extractall(path=data_dir)\n",
    "        \n",
    "        tar_path.unlink()  # Clean up intermediate file\n",
    "        \n",
    "        # Process raw data\n",
    "        print(\"üîÑ Processing raw files...\")\n",
    "        subprocess.run([\n",
    "            'python', 'scripts/preprocess_ptb.py',\n",
    "            '--ptb_root', str(data_dir / 'LDC99T42'),\n",
    "            '--output_dir', str(data_dir),\n",
    "            '--verify'\n",
    "        ], check=True)\n",
    "        \n",
    "        # Cleanup extracted directory\n",
    "        if (data_dir / 'LDC99T42').exists():\n",
    "            shutil.rmtree(data_dir / 'LDC99T42')\n",
    "        \n",
    "        print(\"‚úÖ Data processing completed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Archive not found: {archive_path}\")\n",
    "\n",
    "print(\"\\nüéâ Data is ready for analysis and training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56aae12",
   "metadata": {},
   "source": [
    "## 4. Data Exploration\n",
    "\n",
    "Let's explore the Penn Treebank dataset to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the datasets\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_analyze_data(file_path, name):\n",
    "    \"\"\"Load data and return basic statistics\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    words = text.split()\n",
    "    sentences = text.split('\\n')\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'total_tokens': len(words),\n",
    "        'unique_tokens': len(set(words)),\n",
    "        'sentences': len(sentences),\n",
    "        'avg_sentence_length': len(words) / len(sentences),\n",
    "        'vocabulary': Counter(words)\n",
    "    }\n",
    "\n",
    "# Analyze all splits\n",
    "data_stats = []\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "for split in splits:\n",
    "    file_path = data_dir / f'ptb.{split}.txt'\n",
    "    if file_path.exists():\n",
    "        stats = load_and_analyze_data(file_path, split)\n",
    "        data_stats.append(stats)\n",
    "        print(f\"üìä {split.upper()} SET:\")\n",
    "        print(f\"   Tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"   Vocabulary: {stats['unique_tokens']:,}\")\n",
    "        print(f\"   Sentences: {stats['sentences']:,}\")\n",
    "        print(f\"   Avg sentence length: {stats['avg_sentence_length']:.1f}\")\n",
    "        print()\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'Split': stats['name'].title(),\n",
    "        'Tokens': stats['total_tokens'],\n",
    "        'Vocabulary': stats['unique_tokens'],\n",
    "        'Sentences': stats['sentences'],\n",
    "        'Avg Length': round(stats['avg_sentence_length'], 1)\n",
    "    }\n",
    "    for stats in data_stats\n",
    "])\n",
    "\n",
    "print(\"üìã DATASET SUMMARY:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Penn Treebank Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Dataset sizes\n",
    "ax1 = axes[0, 0]\n",
    "splits = summary_df['Split']\n",
    "tokens = summary_df['Tokens']\n",
    "bars = ax1.bar(splits, tokens, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax1.set_title('Dataset Sizes (Tokens)')\n",
    "ax1.set_ylabel('Number of Tokens')\n",
    "for bar, value in zip(bars, tokens):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(tokens)*0.01,\n",
    "             f'{value:,}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Vocabulary distribution\n",
    "ax2 = axes[0, 1]\n",
    "vocab_sizes = summary_df['Vocabulary']\n",
    "bars = ax2.bar(splits, vocab_sizes, color=['#d62728', '#9467bd', '#8c564b'])\n",
    "ax2.set_title('Vocabulary Sizes')\n",
    "ax2.set_ylabel('Unique Tokens')\n",
    "for bar, value in zip(bars, vocab_sizes):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(vocab_sizes)*0.01,\n",
    "             f'{value:,}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Word frequency distribution (using train set)\n",
    "ax3 = axes[1, 0]\n",
    "train_vocab = data_stats[0]['vocabulary']\n",
    "freq_counts = Counter(train_vocab.values())\n",
    "frequencies = list(freq_counts.keys())[:20]  # Top 20 frequency bins\n",
    "counts = [freq_counts[f] for f in frequencies]\n",
    "\n",
    "ax3.bar(range(len(frequencies)), counts, color='#17becf')\n",
    "ax3.set_title('Word Frequency Distribution (Train)')\n",
    "ax3.set_xlabel('Word Frequency')\n",
    "ax3.set_ylabel('Number of Words')\n",
    "ax3.set_xticks(range(0, len(frequencies), 5))\n",
    "ax3.set_xticklabels([frequencies[i] for i in range(0, len(frequencies), 5)])\n",
    "\n",
    "# 4. Most common words\n",
    "ax4 = axes[1, 1]\n",
    "top_words = train_vocab.most_common(15)\n",
    "words, counts = zip(*top_words)\n",
    "bars = ax4.barh(range(len(words)), counts, color='#bcbd22')\n",
    "ax4.set_title('Most Common Words (Train)')\n",
    "ax4.set_xlabel('Frequency')\n",
    "ax4.set_yticks(range(len(words)))\n",
    "ax4.set_yticklabels(words)\n",
    "ax4.invert_yaxis()\n",
    "\n",
    "# Add frequency labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax4.text(bar.get_width() + max(counts)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "             f'{count:,}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Key Insights:\")\n",
    "print(f\"‚Ä¢ Training set has {tokens[0]:,} tokens - largest portion for learning\")\n",
    "print(f\"‚Ä¢ Vocabulary overlap indicates good data consistency\")\n",
    "print(f\"‚Ä¢ Average sentence length of {summary_df['Avg Length'].mean():.1f} words\")\n",
    "print(f\"‚Ä¢ High-frequency words ('{top_words[0][0]}', '{top_words[1][0]}') dominate the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdcbb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mounting and Pre-trained Model Loading\n",
    "if skip_training:\n",
    "    print(\"üìÇ Setting up pre-trained model loading...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"üîó Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "        \n",
    "        # Set up model loading paths\n",
    "        gdrive_models_path = '/content/drive/MyDrive/PTB_Models'\n",
    "        print(f\"\\nüìÅ Looking for models in: {gdrive_models_path}\")\n",
    "        \n",
    "        # Create models directory if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs(gdrive_models_path, exist_ok=True)\n",
    "        \n",
    "        # Check for existing models\n",
    "        if os.path.exists(gdrive_models_path):\n",
    "            model_files = [f for f in os.listdir(gdrive_models_path) if f.endswith(('.pth', '.pt'))]\n",
    "            \n",
    "            if model_files:\n",
    "                print(f\"\\nüéØ Found {len(model_files)} model(s):\")\n",
    "                for i, model_file in enumerate(model_files, 1):\n",
    "                    file_path = os.path.join(gdrive_models_path, model_file)\n",
    "                    file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "                    print(f\"   {i}. {model_file} ({file_size:.1f} MB)\")\n",
    "                \n",
    "                # Let user select model\n",
    "                while True:\n",
    "                    try:\n",
    "                        choice = input(f\"\\nSelect model (1-{len(model_files)}) or 'u' to upload new: \").strip().lower()\n",
    "                        \n",
    "                        if choice == 'u':\n",
    "                            print(\"\\nüì§ Upload a model file using the file browser on the left\")\n",
    "                            print(\"   or place your model in Google Drive at:\")\n",
    "                            print(f\"   {gdrive_models_path}\")\n",
    "                            print(\"\\nüí° Then re-run this cell to load the model.\")\n",
    "                            pretrained_model_path = None\n",
    "                            break\n",
    "                        else:\n",
    "                            choice_idx = int(choice) - 1\n",
    "                            if 0 <= choice_idx < len(model_files):\n",
    "                                pretrained_model_path = os.path.join(gdrive_models_path, model_files[choice_idx])\n",
    "                                print(f\"‚úÖ Selected: {model_files[choice_idx]}\")\n",
    "                                break\n",
    "                            else:\n",
    "                                print(f\"‚ùå Invalid choice. Enter 1-{len(model_files)} or 'u'\")\n",
    "                    except ValueError:\n",
    "                        print(f\"‚ùå Invalid input. Enter a number 1-{len(model_files)} or 'u'\")\n",
    "            else:\n",
    "                print(\"\\nüì§ No models found in Google Drive.\")\n",
    "                print(\"   Options:\")\n",
    "                print(\"   1. Upload a model file to the Colab file browser\")\n",
    "                print(f\"   2. Place your model in: {gdrive_models_path}\")\n",
    "                print(\"   3. Re-run this cell after uploading\")\n",
    "                pretrained_model_path = None\n",
    "        \n",
    "        # Alternative: Upload via Colab file interface\n",
    "        if 'pretrained_model_path' not in locals() or pretrained_model_path is None:\n",
    "            print(\"\\nüì§ Alternative: Upload model directly\")\n",
    "            upload_choice = input(\"Upload a model file now? (y/n): \").strip().lower()\n",
    "            \n",
    "            if upload_choice == 'y':\n",
    "                from google.colab import files\n",
    "                print(\"\\nüìÅ Select your model file (.pth or .pt):\")\n",
    "                uploaded = files.upload()\n",
    "                \n",
    "                if uploaded:\n",
    "                    # Get the uploaded file\n",
    "                    uploaded_file = list(uploaded.keys())[0]\n",
    "                    pretrained_model_path = uploaded_file\n",
    "                    print(f\"‚úÖ Uploaded: {uploaded_file}\")\n",
    "                else:\n",
    "                    print(\"‚ùå No file uploaded.\")\n",
    "                    pretrained_model_path = None\n",
    "            else:\n",
    "                pretrained_model_path = None\n",
    "                \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Google Colab not detected. Skipping Google Drive mount.\")\n",
    "        print(\"   You can still upload models manually using the file browser.\")\n",
    "        pretrained_model_path = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping pre-trained model loading (training selected)\")\n",
    "    pretrained_model_path = None\n",
    "\n",
    "# Set up model path for later use\n",
    "if skip_training and 'pretrained_model_path' in locals() and pretrained_model_path:\n",
    "    print(f\"\\n‚úÖ Pre-trained model ready: {os.path.basename(pretrained_model_path)}\")\n",
    "    # Copy to local checkpoints for consistency\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    import shutil\n",
    "    local_model_path = f\"checkpoints/{os.path.basename(pretrained_model_path)}\"\n",
    "    shutil.copy2(pretrained_model_path, local_model_path)\n",
    "    print(f\"üìÅ Model copied to: {local_model_path}\")\n",
    "else:\n",
    "    pretrained_model_path = None\n",
    "    \n",
    "print(\"\\nüéØ Ready to proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adab40",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train the LSTM language model with optimized configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdeabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration Selection\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"üéØ Training Configuration Selection\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Choose your training approach:\")\n",
    "print(\"\")\n",
    "print(\"1Ô∏è‚É£ QUICK TRAINING (~5-10 minutes)\")\n",
    "print(\"   ‚Ä¢ Small model, few epochs\")\n",
    "print(\"   ‚Ä¢ Good for testing and demonstrations\")\n",
    "print(\"   ‚Ä¢ Lower final performance\")\n",
    "print(\"\")\n",
    "print(\"2Ô∏è‚É£ FULL TRAINING (~30-60 minutes)\")\n",
    "print(\"   ‚Ä¢ Larger model, more epochs\")\n",
    "print(\"   ‚Ä¢ Better final performance\")\n",
    "print(\"   ‚Ä¢ Requires more time and compute\")\n",
    "print(\"\")\n",
    "print(\"3Ô∏è‚É£ SKIP TRAINING\")\n",
    "print(\"   ‚Ä¢ Load pre-trained model from Google Drive\")\n",
    "print(\"   ‚Ä¢ Skip directly to evaluation and generation\")\n",
    "print(\"   ‚Ä¢ Requires existing trained model\")\n",
    "print(\"\")\n",
    "\n",
    "# Get user choice\n",
    "while True:\n",
    "    choice = input(\"Enter your choice (1, 2, or 3): \").strip()\n",
    "    \n",
    "    if choice == '1':\n",
    "        TRAINING_CONFIG = 'config/config_colab_quick.yaml'\n",
    "        TRAINING_MODE = 'quick'\n",
    "        SKIP_TRAINING = False\n",
    "        print(\"‚úÖ Selected: Quick training\")\n",
    "        break\n",
    "    elif choice == '2':\n",
    "        TRAINING_CONFIG = 'config/config_colab_full.yaml'\n",
    "        TRAINING_MODE = 'full'\n",
    "        SKIP_TRAINING = False\n",
    "        print(\"‚úÖ Selected: Full training\")\n",
    "        break\n",
    "    elif choice == '3':\n",
    "        TRAINING_CONFIG = None\n",
    "        TRAINING_MODE = 'skip'\n",
    "        SKIP_TRAINING = True\n",
    "        print(\"‚úÖ Selected: Skip training - will load pre-trained model\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"‚ùå Invalid choice. Please enter 1, 2, or 3.\")\n",
    "\n",
    "print(f\"\\nüîß Configuration: {TRAINING_MODE}\")\n",
    "if not SKIP_TRAINING:\n",
    "    print(f\"üìù Config file: {TRAINING_CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mount & Pre-trained Model Loading (Optional)\n",
    "if SKIP_TRAINING:\n",
    "    print(\"üìÇ Google Drive Mount & Model Loading\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"üì± Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "        \n",
    "        # List available models\n",
    "        import glob\n",
    "        gdrive_models = glob.glob('/content/drive/MyDrive/**/*.pth', recursive=True) + \\\n",
    "                       glob.glob('/content/drive/MyDrive/**/*.pt', recursive=True)\n",
    "        \n",
    "        if gdrive_models:\n",
    "            print(f\"\\nüìÅ Found {len(gdrive_models)} model file(s) in Google Drive:\")\n",
    "            for i, model_path in enumerate(gdrive_models, 1):\n",
    "                model_name = os.path.basename(model_path)\n",
    "                model_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n",
    "                relative_path = model_path.replace('/content/drive/MyDrive/', '')\n",
    "                print(f\"   {i}. {model_name} ({model_size:.1f} MB) - {relative_path}\")\n",
    "            \n",
    "            # Let user select model\n",
    "            while True:\n",
    "                try:\n",
    "                    model_choice = input(f\"\\nSelect model (1-{len(gdrive_models)}) or 'upload' to upload new: \").strip().lower()\n",
    "                    \n",
    "                    if model_choice == 'upload':\n",
    "                        print(\"üì§ Upload a model file:\")\n",
    "                        from google.colab import files\n",
    "                        uploaded = files.upload()\n",
    "                        \n",
    "                        if uploaded:\n",
    "                            uploaded_file = list(uploaded.keys())[0]\n",
    "                            # Move to checkpoints directory\n",
    "                            os.makedirs('checkpoints', exist_ok=True)\n",
    "                            import shutil\n",
    "                            shutil.move(uploaded_file, f'checkpoints/{uploaded_file}')\n",
    "                            PRETRAINED_MODEL_PATH = f'checkpoints/{uploaded_file}'\n",
    "                            print(f\"‚úÖ Model uploaded and saved: {PRETRAINED_MODEL_PATH}\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"‚ùå No file uploaded. Please try again.\")\n",
    "                    else:\n",
    "                        model_idx = int(model_choice) - 1\n",
    "                        if 0 <= model_idx < len(gdrive_models):\n",
    "                            PRETRAINED_MODEL_PATH = gdrive_models[model_idx]\n",
    "                            print(f\"‚úÖ Selected: {os.path.basename(PRETRAINED_MODEL_PATH)}\")\n",
    "                            \n",
    "                            # Copy to local checkpoints for consistency\n",
    "                            os.makedirs('checkpoints', exist_ok=True)\n",
    "                            local_model_path = f'checkpoints/{os.path.basename(PRETRAINED_MODEL_PATH)}'\n",
    "                            import shutil\n",
    "                            shutil.copy2(PRETRAINED_MODEL_PATH, local_model_path)\n",
    "                            PRETRAINED_MODEL_PATH = local_model_path\n",
    "                            print(f\"üìã Copied to: {local_model_path}\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"‚ùå Invalid choice. Please enter 1-{len(gdrive_models)} or 'upload'.\")\n",
    "                except ValueError:\n",
    "                    print(f\"‚ùå Invalid input. Please enter a number 1-{len(gdrive_models)} or 'upload'.\")\n",
    "        else:\n",
    "            print(\"\\nüì§ No models found in Google Drive. Please upload a model:\")\n",
    "            from google.colab import files\n",
    "            uploaded = files.upload()\n",
    "            \n",
    "            if uploaded:\n",
    "                uploaded_file = list(uploaded.keys())[0]\n",
    "                os.makedirs('checkpoints', exist_ok=True)\n",
    "                import shutil\n",
    "                shutil.move(uploaded_file, f'checkpoints/{uploaded_file}')\n",
    "                PRETRAINED_MODEL_PATH = f'checkpoints/{uploaded_file}'\n",
    "                print(f\"‚úÖ Model uploaded and saved: {PRETRAINED_MODEL_PATH}\")\n",
    "            else:\n",
    "                print(\"‚ùå No model provided. Cannot proceed without a trained model.\")\n",
    "                PRETRAINED_MODEL_PATH = None\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Google Colab not detected. Please upload model file manually:\")\n",
    "        PRETRAINED_MODEL_PATH = input(\"Enter path to your pre-trained model: \").strip()\n",
    "        \n",
    "        if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "            print(f\"‚ùå Model file not found: {PRETRAINED_MODEL_PATH}\")\n",
    "            PRETRAINED_MODEL_PATH = None\n",
    "else:\n",
    "    PRETRAINED_MODEL_PATH = None\n",
    "    print(\"‚è≠Ô∏è Skipping pre-trained model loading (training mode selected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display selected configuration\n",
    "if not SKIP_TRAINING:\n",
    "    config_to_load = TRAINING_CONFIG\n",
    "else:\n",
    "    # Use quick config for model structure when loading pre-trained model\n",
    "    config_to_load = 'config/config_colab_quick.yaml'\n",
    "\n",
    "with open(config_to_load, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"üîß Configuration: {TRAINING_MODE.upper()}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìã Config file: {config_to_load}\")\n",
    "print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
    "print(f\"‚Ä¢ Model type: {config['model']['type']}\")\n",
    "print(f\"‚Ä¢ Hidden size: {config['model']['hidden_dim']}\")\n",
    "print(f\"‚Ä¢ Layers: {config['model']['num_layers']}\")\n",
    "print(f\"‚Ä¢ Dropout: {config['model']['dropout']}\")\n",
    "print(f\"\\n‚öôÔ∏è Training Settings:\")\n",
    "print(f\"‚Ä¢ Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"‚Ä¢ Learning rate: {config['training']['learning_rate']}\")\n",
    "if not SKIP_TRAINING:\n",
    "    print(f\"‚Ä¢ Max epochs: {config['training']['max_epochs']}\")\n",
    "    print(f\"‚Ä¢ Gradient clip: {config['training']['gradient_clip']}\")\n",
    "\n",
    "print(f\"\\nüìä Data Settings:\")\n",
    "print(f\"‚Ä¢ Sequence length: {config['training']['sequence_length']}\")\n",
    "print(f\"‚Ä¢ Min frequency: {config['data']['min_freq']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91836e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Model Training Execution\n",
    "if not SKIP_TRAINING:\n",
    "    print(f\"üöÄ Starting {TRAINING_MODE.upper()} Training\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create checkpoints directory\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Display training info\n",
    "    print(f\"üìã Using config: {TRAINING_CONFIG}\")\n",
    "    print(f\"üéØ Mode: {TRAINING_MODE}\")\n",
    "    \n",
    "    if TRAINING_MODE == 'quick':\n",
    "        print(\"‚ö° Quick training mode - faster but lower performance\")\n",
    "        print(\"üïí Estimated time: 5-10 minutes\")\n",
    "    else:\n",
    "        print(\"üèÜ Full training mode - better performance but takes longer\")\n",
    "        print(\"üï∞Ô∏è Estimated time: 30-60 minutes\")\n",
    "    \n",
    "    print(\"\\nüèÅ Starting training...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute training with selected configuration\n",
    "    !python src/train.py --config {TRAINING_CONFIG} --device cuda\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Check if training completed successfully\n",
    "    checkpoint_files = []\n",
    "    if os.path.exists('checkpoints'):\n",
    "        checkpoint_files = [f for f in os.listdir('checkpoints') if f.endswith(('.pth', '.pt'))]\n",
    "    \n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = max([os.path.join('checkpoints', f) for f in checkpoint_files], \n",
    "                               key=os.path.getctime)\n",
    "        MODEL_PATH = latest_checkpoint\n",
    "        print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "        print(f\"‚è±Ô∏è Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
    "        print(f\"üìÅ Model saved: {os.path.basename(MODEL_PATH)}\")\n",
    "        print(f\"üìä TensorBoard logs: runs/\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Training failed - no model checkpoints found\")\n",
    "        MODEL_PATH = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping training - using pre-trained model\")\n",
    "    MODEL_PATH = PRETRAINED_MODEL_PATH\n",
    "    if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "        print(f\"‚úÖ Using pre-trained model: {os.path.basename(MODEL_PATH)}\")\n",
    "        \n",
    "        # Display model info\n",
    "        checkpoint_info = torch.load(MODEL_PATH, map_location='cpu')\n",
    "        if 'epoch' in checkpoint_info:\n",
    "            print(f\"üìä Model trained for {checkpoint_info['epoch']} epochs\")\n",
    "        if 'loss' in checkpoint_info:\n",
    "            print(f\"üìâ Final loss: {checkpoint_info['loss']:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ùå No valid pre-trained model available\")\n",
    "        MODEL_PATH = None\n",
    "\n",
    "print(f\"\\nüéØ Ready for evaluation and text generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134598a6",
   "metadata": {},
   "source": [
    "## 6. Training Monitoring\n",
    "\n",
    "Launch TensorBoard to visualize training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs --host 0.0.0.0\n",
    "\n",
    "print(\"üìä TensorBoard launched above ‚òùÔ∏è\")\n",
    "print(\"Monitor training/validation loss and perplexity in real-time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370367ac",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66feb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"üßÆ Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    print(f\"üìÅ Using model: {os.path.basename(MODEL_PATH)}\")\n",
    "    \n",
    "    # Determine config file to use\n",
    "    if SKIP_TRAINING:\n",
    "        eval_config = 'config/config_colab_quick.yaml'  # Default for structure\n",
    "    else:\n",
    "        eval_config = TRAINING_CONFIG\n",
    "    \n",
    "    print(f\"üìã Using config: {eval_config}\")\n",
    "    print(\"\\nüîç Evaluating model on test set...\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    !python src/evaluate.py --model_path {MODEL_PATH} --config {eval_config} --device cuda\n",
    "    \n",
    "    print(\"\\n‚úÖ Evaluation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained model found. Please:\")\n",
    "    if SKIP_TRAINING:\n",
    "        print(\"   ‚Ä¢ Ensure you loaded a valid pre-trained model\")\n",
    "        print(\"   ‚Ä¢ Re-run the model loading cell if needed\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Complete training first\")\n",
    "        print(\"   ‚Ä¢ Check if training completed successfully\")\n",
    "    \n",
    "# Set checkpoint files for other cells\n",
    "if MODEL_PATH:\n",
    "    checkpoint_files = [MODEL_PATH]\n",
    "    latest_checkpoint = MODEL_PATH\n",
    "else:\n",
    "    checkpoint_files = []\n",
    "    latest_checkpoint = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90575fb7",
   "metadata": {},
   "source": [
    "## 8. Interactive Text Generation\n",
    "\n",
    "Now let's have some fun! Generate text interactively with custom seed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b35caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up interactive text generation\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from model import create_model\n",
    "from data_loader import Vocabulary\n",
    "from utils import load_checkpoint, generate_text\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load vocabulary\n",
    "with open('data/ptb/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(f\"üìñ Vocabulary loaded: {len(vocab)} words\")\n",
    "\n",
    "# Load model\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    print(f\"üîÑ Loading model: {os.path.basename(MODEL_PATH)}\")\n",
    "    \n",
    "    # Create model architecture (use appropriate config)\n",
    "    if SKIP_TRAINING:\n",
    "        config_file = 'config/config_colab_quick.yaml'  # Default structure\n",
    "    else:\n",
    "        config_file = TRAINING_CONFIG\n",
    "    \n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    model = create_model(config, len(vocab))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load trained weights\n",
    "    model, _, _, _ = load_checkpoint(MODEL_PATH, model)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"‚úÖ Model loaded and ready for text generation!\")\n",
    "    \n",
    "    def interactive_generate(seed_text=\"the\", max_length=50, temperature=1.0):\n",
    "        \"\"\"Generate text with custom parameters\"\"\"\n",
    "        print(f\"\\nüéØ Generating text...\")\n",
    "        print(f\"Seed: '{seed_text}' | Length: {max_length} | Temperature: {temperature}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            generated = generate_text(\n",
    "                model=model,\n",
    "                vocab=vocab,\n",
    "                seed_text=seed_text,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            print(f\"üìù Generated text:\\n{generated}\")\n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Generation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Demonstrate different generation styles\n",
    "    print(\"üé® Text Generation Examples:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Conservative generation (low temperature)\n",
    "    print(\"1Ô∏è‚É£ Conservative (temperature=0.7):\")\n",
    "    interactive_generate(\"the economy\", max_length=40, temperature=0.7)\n",
    "    \n",
    "    # Balanced generation\n",
    "    print(\"\\n2Ô∏è‚É£ Balanced (temperature=1.0):\")\n",
    "    interactive_generate(\"financial markets\", max_length=40, temperature=1.0)\n",
    "    \n",
    "    # Creative generation (high temperature)\n",
    "    print(\"\\n3Ô∏è‚É£ Creative (temperature=1.3):\")\n",
    "    interactive_generate(\"investors\", max_length=40, temperature=1.3)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained model available.\")\n",
    "    if SKIP_TRAINING:\n",
    "        print(\"   ‚Ä¢ Please load a pre-trained model first\")\n",
    "        print(\"   ‚Ä¢ Re-run the model loading cell\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Please complete training first\")\n",
    "        print(\"   ‚Ä¢ Check if training completed successfully\")\n",
    "    \n",
    "    # Create dummy function for error handling\n",
    "    def interactive_generate(seed_text=\"the\", max_length=50, temperature=1.0):\n",
    "        print(\"‚ùå No model available for text generation\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive text generation widget\n",
    "print(\"üéÆ Interactive Text Generation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    print(\"Customize your text generation below:\")\n",
    "    \n",
    "    # Simple interactive interface\n",
    "    def generate_custom_text():\n",
    "        \"\"\"Interactive text generation function\"\"\"\n",
    "        \n",
    "        print(\"\\nüìù Enter your parameters:\")\n",
    "        \n",
    "        # Get user inputs\n",
    "        seed_text = input(\"Seed text (e.g., 'the market'): \").strip() or \"the\"\n",
    "        \n",
    "        try:\n",
    "            max_length = int(input(\"Maximum length (10-100): \") or \"30\")\n",
    "            max_length = max(10, min(100, max_length))  # Clamp between 10-100\n",
    "        except ValueError:\n",
    "            max_length = 30\n",
    "            print(f\"Using default length: {max_length}\")\n",
    "        \n",
    "        try:\n",
    "            temperature = float(input(\"Temperature (0.5-2.0, higher=more creative): \") or \"1.0\")\n",
    "            temperature = max(0.1, min(2.0, temperature))  # Clamp between 0.1-2.0\n",
    "        except ValueError:\n",
    "            temperature = 1.0\n",
    "            print(f\"Using default temperature: {temperature}\")\n",
    "        \n",
    "        # Generate text\n",
    "        generated = interactive_generate(seed_text, max_length, temperature)\n",
    "        \n",
    "        if generated:\n",
    "            print(f\"\\n‚ú® Want to try again? Call generate_custom_text() or try different parameters!\")\n",
    "        \n",
    "        return generated\n",
    "    \n",
    "    # Instructions for use\n",
    "    print(\"\\nüí° Instructions:\")\n",
    "    print(\"‚Ä¢ Call generate_custom_text() to start interactive generation\")\n",
    "    print(\"‚Ä¢ Or use interactive_generate('your seed', length, temperature) directly\")\n",
    "    print(\"‚Ä¢ Examples:\")\n",
    "    print(\"  - interactive_generate('the stock market', 50, 1.0)\")\n",
    "    print(\"  - interactive_generate('investors', 30, 0.8)\")\n",
    "    \n",
    "    # Ready message\n",
    "    print(f\"\\nüéØ Model ready: {os.path.basename(MODEL_PATH)}\")\n",
    "    print(\"Ready for text generation! üöÄ\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model available for interactive generation.\")\n",
    "    if SKIP_TRAINING:\n",
    "        print(\"   ‚Ä¢ Please load a pre-trained model first\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Please complete training first\")\n",
    "    \n",
    "    def generate_custom_text():\n",
    "        print(\"‚ùå No model available. Please load a model first.\")\n",
    "        return None\n",
    "\n",
    "# Instructions for users\n",
    "print(\"\\nüí° Tips for text generation:\")\n",
    "print(\"‚Ä¢ Lower temperature (0.5-0.8): More predictable, grammatical text\")\n",
    "print(\"‚Ä¢ Higher temperature (1.2-2.0): More creative, diverse text\")\n",
    "print(\"‚Ä¢ Seed text: Use Penn Treebank vocabulary for best results\")\n",
    "print(\"‚Ä¢ Common seeds: 'the', 'market', 'company', 'economic', 'financial'\")\n",
    "\n",
    "print(\"\\nüëá Run the cell below to start interactive generation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7915f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run interactive generation\n",
    "generate_custom_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a53b0",
   "metadata": {},
   "source": [
    "## 9. Results and Conclusion\n",
    "\n",
    "Summary of training results and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize results\n",
    "print(\"üìä Training Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Get checkpoint info\n",
    "    checkpoint_info = torch.load(latest_checkpoint, map_location='cpu')\n",
    "    \n",
    "    print(f\"‚úÖ Model trained successfully!\")\n",
    "    print(f\"üìÅ Best model: {os.path.basename(latest_checkpoint)}\")\n",
    "    print(f\"üî¢ Final epoch: {checkpoint_info.get('epoch', 'Unknown')}\")\n",
    "    print(f\"üìâ Final loss: {checkpoint_info.get('loss', 'Unknown'):.4f}\")\n",
    "    \n",
    "    # Model architecture summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
    "    print(f\"‚Ä¢ Total parameters: {total_params:,}\")\n",
    "    print(f\"‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"‚Ä¢ Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    print(f\"\\nüéØ Key Results:\")\n",
    "    print(f\"‚Ä¢ Successfully trained LSTM language model\")\n",
    "    print(f\"‚Ä¢ Model can generate coherent text sequences\")\n",
    "    print(f\"‚Ä¢ Interactive generation with customizable parameters\")\n",
    "    print(f\"‚Ä¢ Ready for deployment or further fine-tuning\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No trained model found.\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "if os.path.exists('checkpoints') and os.listdir('checkpoints'):\n",
    "    print(f\"‚Ä¢ Checkpoints: {len(os.listdir('checkpoints'))} files\")\n",
    "if os.path.exists('runs') and os.listdir('runs'):\n",
    "    print(f\"‚Ä¢ TensorBoard logs: Available in runs/\")\n",
    "\n",
    "print(f\"\\nüéâ Training and evaluation completed!\")\n",
    "print(f\"You can now experiment with different seed texts and parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282905c",
   "metadata": {},
   "source": [
    "## 10. Download Results (Optional)\n",
    "\n",
    "Package and download your training results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8107ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "def package_results():\n",
    "    \"\"\"Package all generated files for download\"\"\"\n",
    "    \n",
    "    print(\"üì¶ Packaging results...\")\n",
    "    \n",
    "    items_to_include = []\n",
    "    \n",
    "    # Check what we have\n",
    "    if os.path.exists('checkpoints') and os.listdir('checkpoints'):\n",
    "        checkpoint_count = len([f for f in os.listdir('checkpoints') if f.endswith(('.pth', '.pt'))])\n",
    "        if checkpoint_count > 0:\n",
    "            items_to_include.append(('checkpoints', f'Model checkpoints ({checkpoint_count} files)'))\n",
    "    \n",
    "    if os.path.exists('runs') and os.listdir('runs'):\n",
    "        items_to_include.append(('runs', 'TensorBoard logs'))\n",
    "    \n",
    "    # Include vocabulary and config files\n",
    "    if os.path.exists('data/ptb/vocab.pkl'):\n",
    "        items_to_include.append(('data/ptb/vocab.pkl', 'Vocabulary file'))\n",
    "    \n",
    "    # Include relevant config file\n",
    "    if not SKIP_TRAINING and TRAINING_CONFIG and os.path.exists(TRAINING_CONFIG):\n",
    "        items_to_include.append((TRAINING_CONFIG, 'Training configuration'))\n",
    "    \n",
    "    if not items_to_include:\n",
    "        print(\"‚ö†Ô∏è No results to package.\")\n",
    "        if SKIP_TRAINING:\n",
    "            print(\"   ‚Ä¢ Pre-trained model mode - no training artifacts to package\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ Run training first to generate results\")\n",
    "        return None\n",
    "    \n",
    "    # Create results archive\n",
    "    zip_filename = f'penn_treebank_results_{TRAINING_MODE}.zip'\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for item_path, description in items_to_include:\n",
    "            print(f\"   üìÅ Adding {description}...\")\n",
    "            \n",
    "            if os.path.isfile(item_path):\n",
    "                # Single file\n",
    "                zipf.write(item_path, os.path.basename(item_path))\n",
    "            else:\n",
    "                # Directory\n",
    "                for root, dirs, files in os.walk(item_path):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arcname = os.path.relpath(file_path, '.')\n",
    "                        zipf.write(file_path, arcname)\n",
    "    \n",
    "    print(f\"‚úÖ Results packaged in: {zip_filename}\")\n",
    "    return zip_filename\n",
    "\n",
    "# Package and download\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    zip_file = package_results()\n",
    "    \n",
    "    if zip_file:\n",
    "        print(\"\\n‚¨áÔ∏è Downloading results...\")\n",
    "        try:\n",
    "            files.download(zip_file)\n",
    "            print(\"‚úÖ Download started! Check your browser's download folder.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Download failed: {e}\")\n",
    "            print(f\"   ‚Ä¢ File saved locally as: {zip_file}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No results to download.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training results to download.\")\n",
    "    if SKIP_TRAINING:\n",
    "        print(\"   ‚Ä¢ Pre-trained model mode - no training artifacts to download\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Complete training first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290560b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "**Experiment Further:**\n",
    "- Try different model architectures (GRU, Transformer)\n",
    "- Experiment with hyperparameters in config files\n",
    "- Train on different datasets\n",
    "- Implement beam search for better text generation\n",
    "\n",
    "**Deploy Your Model:**\n",
    "- Export model for production use\n",
    "- Create a web interface for text generation\n",
    "- Fine-tune on domain-specific text\n",
    "\n",
    "**Learn More:**\n",
    "- Study attention mechanisms\n",
    "- Explore modern transformer architectures\n",
    "- Experiment with different text generation techniques\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrated complete language modeling pipeline from data exploration to interactive text generation. Feel free to modify and extend it for your own experiments!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
