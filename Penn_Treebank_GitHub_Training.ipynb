{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abmichael/penn-treebank-ptb/blob/main/Penn_Treebank_GitHub_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7475a970",
      "metadata": {
        "id": "7475a970"
      },
      "source": [
        "# Penn Treebank Language Modeling\n",
        "*A complete implementation of LSTM-based language modeling on the Penn Treebank dataset*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d23b7e",
      "metadata": {
        "id": "06d23b7e"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates end-to-end language modeling on the Penn Treebank dataset with the following features:\n",
        "\n",
        "**üéØ Key Features:**\n",
        "- **GitHub Integration**: Project loaded directly from repository\n",
        "- **Zero Setup**: No manual uploads required\n",
        "- **GPU Acceleration**: Optimized for Colab GPU training\n",
        "- **Interactive Evaluation**: Generate text with custom seed words\n",
        "- **Comprehensive Analysis**: Data exploration and model evaluation\n",
        "\n",
        "**‚öôÔ∏è Requirements:**\n",
        "- Google Colab with GPU runtime\n",
        "- Public GitHub repository (automatically cloned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d9078bc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9078bc1",
        "outputId": "7de11884-9211-45cd-fcdc-63581034fa46"
      },
      "outputs": [],
      "source": [
        "# Configuration - Update with your GitHub repository\n",
        "GITHUB_REPO = \"https://github.com/Abmichael/penn-treebank-ptb.git\"\n",
        "PROJECT_NAME = \"penn-treebank-ptb\"\n",
        "\n",
        "print(\"üöÄ Penn Treebank Language Modeling\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5317d02",
      "metadata": {
        "id": "c5317d02"
      },
      "source": [
        "## 1. Project Setup\n",
        "\n",
        "First, we'll clone the project from GitHub and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5e489e63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e489e63",
        "outputId": "84fd427c-1f27-4243-dca1-edfcd9e3e009"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Clone project from GitHub\n",
        "if not os.path.exists(PROJECT_NAME):\n",
        "    print(\"üì¶ Cloning project from GitHub...\")\n",
        "    try:\n",
        "        result = subprocess.run(['git', 'clone', GITHUB_REPO],\n",
        "                              capture_output=True, text=True, check=True)\n",
        "        print(\"‚úÖ Project cloned successfully!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Failed to clone repository: {e}\")\n",
        "        print(\"Please ensure the repository URL is correct and publicly accessible\")\n",
        "        sys.exit(1)\n",
        "else:\n",
        "    print(\"‚úÖ Project already exists\")\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(PROJECT_NAME)\n",
        "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Create local directories for generated files\n",
        "local_dirs = ['checkpoints', 'runs', 'logs', 'outputs']\n",
        "for dir_name in local_dirs:\n",
        "    os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "print(\"\\nüìÇ Storage Strategy:\")\n",
        "print(\"   ‚Ä¢ Source code: GitHub repository\")\n",
        "print(\"   ‚Ä¢ Generated files: Local session storage\")\n",
        "print(\"   ‚Ä¢ Results: Available for download\")\n",
        "\n",
        "# Verify project structure\n",
        "required_items = ['src', 'config', 'data', 'requirements.txt']\n",
        "missing_items = [item for item in required_items if not os.path.exists(item)]\n",
        "\n",
        "if missing_items:\n",
        "    print(f\"‚ö†Ô∏è Missing required items: {missing_items}\")\n",
        "else:\n",
        "    print(\"‚úÖ Project structure verified!\")\n",
        "    print(f\"üìã Contents: {', '.join(os.listdir('.'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b95e44b1",
      "metadata": {
        "id": "b95e44b1"
      },
      "source": [
        "## 2. Environment Setup\n",
        "\n",
        "Install dependencies and verify GPU availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a94578ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a94578ea",
        "outputId": "a2c61523-d9b3-4351-f77b-bde1fa569812"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# Import required libraries and check GPU\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "print(\"\\nüîç Environment Information:\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "    print(\"‚úÖ GPU is ready for training!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU not detected. Using CPU (will be slower)\")\n",
        "    print(\"üí° Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1703c883",
      "metadata": {
        "id": "1703c883"
      },
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "Extract and process the Penn Treebank dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfeb3526",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfeb3526",
        "outputId": "9ba8a7e1-ea66-4dd6-eb71-dc111a3b2867"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import subprocess\n",
        "import time\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if data needs extraction\n",
        "data_dir = Path('data/ptb')\n",
        "archive_path = data_dir / 'LDC99T42_Penn_Treebank_3.tar.zst'\n",
        "processed_files = ['ptb.train.txt', 'ptb.valid.txt', 'ptb.test.txt']\n",
        "\n",
        "# Check if processed files exist\n",
        "all_processed_exist = all((data_dir / f).exists() for f in processed_files)\n",
        "\n",
        "if all_processed_exist:\n",
        "    print(\"‚úÖ Penn Treebank data already processed\")\n",
        "    for f in processed_files:\n",
        "        file_path = data_dir / f\n",
        "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"   üìÑ {f}: {size_mb:.1f} MB\")\n",
        "else:\n",
        "    print(\"‚ùóNO DATA...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56aae12",
      "metadata": {
        "id": "a56aae12"
      },
      "source": [
        "## 4. Data Exploration\n",
        "\n",
        "Let's explore the Penn Treebank dataset to understand its characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "532f76cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "532f76cd",
        "outputId": "6273538a-395d-4b4a-a314-e559aa1766ca"
      },
      "outputs": [],
      "source": [
        "# Load and analyze the datasets\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def load_and_analyze_data(file_path, name):\n",
        "    \"\"\"Load data and return basic statistics\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    words = text.split()\n",
        "    sentences = text.split('\\n')\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'total_tokens': len(words),\n",
        "        'unique_tokens': len(set(words)),\n",
        "        'sentences': len(sentences),\n",
        "        'avg_sentence_length': len(words) / len(sentences),\n",
        "        'vocabulary': Counter(words)\n",
        "    }\n",
        "\n",
        "# Analyze all splits\n",
        "data_stats = []\n",
        "splits = ['train', 'valid', 'test']\n",
        "\n",
        "for split in splits:\n",
        "    file_path = data_dir / f'ptb.{split}.txt'\n",
        "    if file_path.exists():\n",
        "        stats = load_and_analyze_data(file_path, split)\n",
        "        data_stats.append(stats)\n",
        "        print(f\"üìä {split.upper()} SET:\")\n",
        "        print(f\"   Tokens: {stats['total_tokens']:,}\")\n",
        "        print(f\"   Vocabulary: {stats['unique_tokens']:,}\")\n",
        "        print(f\"   Sentences: {stats['sentences']:,}\")\n",
        "        print(f\"   Avg sentence length: {stats['avg_sentence_length']:.1f}\")\n",
        "        print()\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame([\n",
        "    {\n",
        "        'Split': stats['name'].title(),\n",
        "        'Tokens': stats['total_tokens'],\n",
        "        'Vocabulary': stats['unique_tokens'],\n",
        "        'Sentences': stats['sentences'],\n",
        "        'Avg Length': round(stats['avg_sentence_length'], 1)\n",
        "    }\n",
        "    for stats in data_stats\n",
        "])\n",
        "\n",
        "print(\"üìã DATASET SUMMARY:\")\n",
        "print(summary_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0241210a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0241210a",
        "outputId": "3be8c3c5-ebcf-4f9e-b299-12a69d1c5dac"
      },
      "outputs": [],
      "source": [
        "# Visualize dataset characteristics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Penn Treebank Dataset Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Dataset sizes\n",
        "ax1 = axes[0, 0]\n",
        "splits = summary_df['Split']\n",
        "tokens = summary_df['Tokens']\n",
        "bars = ax1.bar(splits, tokens, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "ax1.set_title('Dataset Sizes (Tokens)')\n",
        "ax1.set_ylabel('Number of Tokens')\n",
        "for bar, value in zip(bars, tokens):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(tokens)*0.01,\n",
        "             f'{value:,}', ha='center', va='bottom')\n",
        "\n",
        "# 2. Vocabulary distribution\n",
        "ax2 = axes[0, 1]\n",
        "vocab_sizes = summary_df['Vocabulary']\n",
        "bars = ax2.bar(splits, vocab_sizes, color=['#d62728', '#9467bd', '#8c564b'])\n",
        "ax2.set_title('Vocabulary Sizes')\n",
        "ax2.set_ylabel('Unique Tokens')\n",
        "for bar, value in zip(bars, vocab_sizes):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(vocab_sizes)*0.01,\n",
        "             f'{value:,}', ha='center', va='bottom')\n",
        "\n",
        "# 3. Word frequency distribution (using train set)\n",
        "ax3 = axes[1, 0]\n",
        "train_vocab = data_stats[0]['vocabulary']\n",
        "freq_counts = Counter(train_vocab.values())\n",
        "frequencies = list(freq_counts.keys())[:20]  # Top 20 frequency bins\n",
        "counts = [freq_counts[f] for f in frequencies]\n",
        "\n",
        "ax3.bar(range(len(frequencies)), counts, color='#17becf')\n",
        "ax3.set_title('Word Frequency Distribution (Train)')\n",
        "ax3.set_xlabel('Word Frequency')\n",
        "ax3.set_ylabel('Number of Words')\n",
        "ax3.set_xticks(range(0, len(frequencies), 5))\n",
        "ax3.set_xticklabels([frequencies[i] for i in range(0, len(frequencies), 5)])\n",
        "\n",
        "# 4. Most common words\n",
        "ax4 = axes[1, 1]\n",
        "top_words = train_vocab.most_common(15)\n",
        "words, counts = zip(*top_words)\n",
        "bars = ax4.barh(range(len(words)), counts, color='#bcbd22')\n",
        "ax4.set_title('Most Common Words (Train)')\n",
        "ax4.set_xlabel('Frequency')\n",
        "ax4.set_yticks(range(len(words)))\n",
        "ax4.set_yticklabels(words)\n",
        "ax4.invert_yaxis()\n",
        "\n",
        "# Add frequency labels\n",
        "for bar, count in zip(bars, counts):\n",
        "    ax4.text(bar.get_width() + max(counts)*0.01, bar.get_y() + bar.get_height()/2,\n",
        "             f'{count:,}', ha='left', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìà Key Insights:\")\n",
        "print(f\"‚Ä¢ Training set has {tokens[0]:,} tokens - largest portion for learning\")\n",
        "print(f\"‚Ä¢ Vocabulary overlap indicates good data consistency\")\n",
        "print(f\"‚Ä¢ Average sentence length of {summary_df['Avg Length'].mean():.1f} words\")\n",
        "print(f\"‚Ä¢ High-frequency words ('{top_words[0][0]}', '{top_words[1][0]}') dominate the corpus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13adab40",
      "metadata": {
        "id": "13adab40"
      },
      "source": [
        "## 5. Model Training\n",
        "\n",
        "Train the LSTM language model with optimized configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afdeabb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afdeabb8",
        "outputId": "510d6c83-c12b-4bc8-d806-dbd9bcd188bc"
      },
      "outputs": [],
      "source": [
        "# Training Configuration Selection - Unified Optimal Config\n",
        "\n",
        "import os\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "\n",
        "print(\"üéØ Unified Optimal Configuration\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"Using one optimal configuration for all scenarios:\")\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"üéØ OPTIMAL TRAINING\")\n",
        "\n",
        "print(\"   ‚Ä¢ Balanced model parameters to prevent overfitting/underfitting\")\n",
        "\n",
        "print(\"   ‚Ä¢ Proven configuration based on PTB best practices\")\n",
        "\n",
        "print(\"   ‚Ä¢ One config to rule them all approach\")\n",
        "\n",
        "print(\"   ‚Ä¢ Estimated training time: ~20-30 minutes\")\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"‚è≠Ô∏è Or SKIP TRAINING to load a pre-trained model\")\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "# Get user choice\n",
        "\n",
        "while True:\n",
        "\n",
        "    choice = input(\"Train with optimal config? (y/n) or 's' to skip: \").strip().lower()\n",
        "\n",
        "\n",
        "\n",
        "    if choice in ['y', 'yes', '1', 'train']:\n",
        "\n",
        "        TRAINING_CONFIG = 'config/optimal_config.yaml'\n",
        "\n",
        "        TRAINING_MODE = 'optimal'\n",
        "\n",
        "        SKIP_TRAINING = False\n",
        "\n",
        "        print(\"‚úÖ Selected: Optimal unified training configuration\")\n",
        "\n",
        "        break\n",
        "\n",
        "    elif choice in ['s', 'skip', '3']:\n",
        "\n",
        "        TRAINING_CONFIG = 'config/optimal_config.yaml'  # Still use optimal for structure\n",
        "\n",
        "        TRAINING_MODE = 'skip'\n",
        "\n",
        "        SKIP_TRAINING = True\n",
        "\n",
        "        print(\"‚úÖ Selected: Skip training - will load pre-trained model\")\n",
        "\n",
        "        break\n",
        "\n",
        "    else:\n",
        "\n",
        "        print(\"‚ùå Invalid choice. Please enter 'y' to train or 's' to skip.\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nüîß Configuration: {TRAINING_MODE}\")\n",
        "\n",
        "print(f\"üìù Config file: {TRAINING_CONFIG}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "df7f4a04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df7f4a04",
        "outputId": "dc8c88c7-07dd-4197-9582-89850c1d7f95"
      },
      "outputs": [],
      "source": [
        "# Google Drive Mount & Pre-trained Model Loading (Optional)\n",
        "if SKIP_TRAINING:\n",
        "    print(\"üìÇ Google Drive Mount & Model Loading\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"üì± Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úÖ Google Drive mounted successfully!\")\n",
        "\n",
        "        # List available models\n",
        "        import glob\n",
        "        gdrive_models = glob.glob('/content/drive/MyDrive/**/*.pth', recursive=True) + \\\n",
        "                       glob.glob('/content/drive/MyDrive/**/*.pt', recursive=True)\n",
        "\n",
        "        if gdrive_models:\n",
        "            print(f\"\\nüìÅ Found {len(gdrive_models)} model file(s) in Google Drive:\")\n",
        "            for i, model_path in enumerate(gdrive_models, 1):\n",
        "                model_name = os.path.basename(model_path)\n",
        "                model_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n",
        "                relative_path = model_path.replace('/content/drive/MyDrive/', '')\n",
        "                print(f\"   {i}. {model_name} ({model_size:.1f} MB) - {relative_path}\")\n",
        "\n",
        "            # Let user select model\n",
        "            while True:\n",
        "                try:\n",
        "                    model_choice = input(f\"\\nSelect model (1-{len(gdrive_models)}) or 'upload' to upload new: \").strip().lower()\n",
        "\n",
        "                    if model_choice == 'upload':\n",
        "                        print(\"üì§ Upload a model file:\")\n",
        "                        from google.colab import files\n",
        "                        uploaded = files.upload()\n",
        "\n",
        "                        if uploaded:\n",
        "                            uploaded_file = list(uploaded.keys())[0]\n",
        "                            # Move to checkpoints directory\n",
        "                            os.makedirs('checkpoints', exist_ok=True)\n",
        "                            import shutil\n",
        "                            shutil.move(uploaded_file, f'checkpoints/{uploaded_file}')\n",
        "                            PRETRAINED_MODEL_PATH = f'checkpoints/{uploaded_file}'\n",
        "                            print(f\"‚úÖ Model uploaded and saved: {PRETRAINED_MODEL_PATH}\")\n",
        "                            break\n",
        "                        else:\n",
        "                            print(\"‚ùå No file uploaded. Please try again.\")\n",
        "                    else:\n",
        "                        model_idx = int(model_choice) - 1\n",
        "                        if 0 <= model_idx < len(gdrive_models):\n",
        "                            PRETRAINED_MODEL_PATH = gdrive_models[model_idx]\n",
        "                            print(f\"‚úÖ Selected: {os.path.basename(PRETRAINED_MODEL_PATH)}\")\n",
        "\n",
        "                            # Copy to local checkpoints for consistency\n",
        "                            os.makedirs('checkpoints', exist_ok=True)\n",
        "                            local_model_path = f'checkpoints/{os.path.basename(PRETRAINED_MODEL_PATH)}'\n",
        "                            import shutil\n",
        "                            shutil.copy2(PRETRAINED_MODEL_PATH, local_model_path)\n",
        "                            PRETRAINED_MODEL_PATH = local_model_path\n",
        "                            print(f\"üìã Copied to: {local_model_path}\")\n",
        "                            break\n",
        "                        else:\n",
        "                            print(f\"‚ùå Invalid choice. Please enter 1-{len(gdrive_models)} or 'upload'.\")\n",
        "                except ValueError:\n",
        "                    print(f\"‚ùå Invalid input. Please enter a number 1-{len(gdrive_models)} or 'upload'.\")\n",
        "        else:\n",
        "            print(\"\\nüì§ No models found in Google Drive. Please upload a model:\")\n",
        "            from google.colab import files\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            if uploaded:\n",
        "                uploaded_file = list(uploaded.keys())[0]\n",
        "                os.makedirs('checkpoints', exist_ok=True)\n",
        "                import shutil\n",
        "                shutil.move(uploaded_file, f'checkpoints/{uploaded_file}')\n",
        "                PRETRAINED_MODEL_PATH = f'checkpoints/{uploaded_file}'\n",
        "                print(f\"‚úÖ Model uploaded and saved: {PRETRAINED_MODEL_PATH}\")\n",
        "            else:\n",
        "                print(\"‚ùå No model provided. Cannot proceed without a trained model.\")\n",
        "                PRETRAINED_MODEL_PATH = None\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è Google Colab not detected. Please upload model file manually:\")\n",
        "        PRETRAINED_MODEL_PATH = input(\"Enter path to your pre-trained model: \").strip()\n",
        "\n",
        "        if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
        "            print(f\"‚ùå Model file not found: {PRETRAINED_MODEL_PATH}\")\n",
        "            PRETRAINED_MODEL_PATH = None\n",
        "else:\n",
        "    PRETRAINED_MODEL_PATH = None\n",
        "    print(\"‚è≠Ô∏è Skipping pre-trained model loading (training mode selected)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85abab33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85abab33",
        "outputId": "625efc29-9e64-4e91-ba2d-996c81f6d432"
      },
      "outputs": [],
      "source": [
        "# Load and display selected configuration\n",
        "if not SKIP_TRAINING:\n",
        "    config_to_load = TRAINING_CONFIG\n",
        "else:\n",
        "    config_to_load = 'config/optimal_config.yaml'\n",
        "\n",
        "with open(config_to_load, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(f\"üîß Configuration: {TRAINING_MODE.upper()}\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìã Config file: {config_to_load}\")\n",
        "print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
        "print(f\"‚Ä¢ Model type: {config['model']['type']}\")\n",
        "print(f\"‚Ä¢ Hidden size: {config['model']['hidden_dim']}\")\n",
        "print(f\"‚Ä¢ Layers: {config['model']['num_layers']}\")\n",
        "print(f\"‚Ä¢ Dropout: {config['model']['dropout']}\")\n",
        "print(f\"\\n‚öôÔ∏è Training Settings:\")\n",
        "print(f\"‚Ä¢ Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"‚Ä¢ Learning rate: {config['training']['learning_rate']}\")\n",
        "if not SKIP_TRAINING:\n",
        "    print(f\"‚Ä¢ Max epochs: {config['training']['max_epochs']}\")\n",
        "    print(f\"‚Ä¢ Gradient clip: {config['training']['gradient_clip']}\")\n",
        "\n",
        "print(f\"\\nüìä Data Settings:\")\n",
        "print(f\"‚Ä¢ Sequence length: {config['training']['sequence_length']}\")\n",
        "print(f\"‚Ä¢ Min frequency: {config['data']['min_freq']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "91836e0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91836e0d",
        "outputId": "9a97ea6a-3805-4373-c8db-703652256060"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Model Training Execution\n",
        "if not SKIP_TRAINING:\n",
        "    print(f\"üöÄ Starting {TRAINING_MODE.upper()} Training\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create checkpoints directory\n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "    # Display training info\n",
        "    print(f\"üìã Using config: {TRAINING_CONFIG}\")\n",
        "    print(f\"üéØ Mode: {TRAINING_MODE}\")\n",
        "\n",
        "    if TRAINING_MODE == 'quick':\n",
        "        print(\"‚ö° Quick training mode - faster but lower performance\")\n",
        "        print(\"üïí Estimated time: 5-10 minutes\")\n",
        "    else:\n",
        "        print(\"üèÜ Full training mode - better performance but takes longer\")\n",
        "        print(\"üï∞Ô∏è Estimated time: 30-60 minutes\")\n",
        "\n",
        "    print(\"\\nüèÅ Starting training...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Execute training with selected configuration\n",
        "    !python src/train.py --config {TRAINING_CONFIG} --device cuda\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    # Check if training completed successfully\n",
        "    checkpoint_files = []\n",
        "    if os.path.exists('checkpoints'):\n",
        "        checkpoint_files = [f for f in os.listdir('checkpoints') if f.endswith(('.pth', '.pt'))]\n",
        "\n",
        "    if checkpoint_files:\n",
        "        latest_checkpoint = max([os.path.join('checkpoints', f) for f in checkpoint_files],\n",
        "                               key=os.path.getctime)\n",
        "        MODEL_PATH = latest_checkpoint\n",
        "        print(f\"\\n‚úÖ Training completed successfully!\")\n",
        "        print(f\"‚è±Ô∏è Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
        "        print(f\"üìÅ Model saved: {os.path.basename(MODEL_PATH)}\")\n",
        "        print(f\"üìä TensorBoard logs: runs/\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Training failed - no model checkpoints found\")\n",
        "        MODEL_PATH = None\n",
        "\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipping training - using pre-trained model\")\n",
        "    MODEL_PATH = PRETRAINED_MODEL_PATH\n",
        "    if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
        "        print(f\"‚úÖ Using pre-trained model: {os.path.basename(MODEL_PATH)}\")\n",
        "\n",
        "        # Display model info\n",
        "        checkpoint_info = torch.load(MODEL_PATH, map_location='cpu')\n",
        "        if 'epoch' in checkpoint_info:\n",
        "            print(f\"üìä Model trained for {checkpoint_info['epoch']} epochs\")\n",
        "        if 'loss' in checkpoint_info:\n",
        "            print(f\"üìâ Final loss: {checkpoint_info['loss']:.4f}\")\n",
        "    else:\n",
        "        print(\"‚ùå No valid pre-trained model available\")\n",
        "        MODEL_PATH = None\n",
        "\n",
        "print(f\"\\nüéØ Ready for evaluation and text generation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134598a6",
      "metadata": {
        "id": "134598a6"
      },
      "source": [
        "## 6. Training Monitoring\n",
        "\n",
        "Launch TensorBoard to visualize training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694f73eb",
      "metadata": {
        "id": "694f73eb"
      },
      "outputs": [],
      "source": [
        "# Launch TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs --host 0.0.0.0\n",
        "\n",
        "print(\"üìä TensorBoard launched above ‚òùÔ∏è\")\n",
        "print(\"Monitor training/validation loss and perplexity in real-time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370367ac",
      "metadata": {
        "id": "370367ac"
      },
      "source": [
        "## 7. Model Evaluation\n",
        "\n",
        "Evaluate the trained model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "66feb3e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66feb3e1",
        "outputId": "d0c2fd6f-3946-447a-974c-cfd41f5182f7"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Model Evaluation\n",
        "print(\"üßÆ Model Evaluation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
        "    print(f\"üìÅ Using model: {os.path.basename(MODEL_PATH)}\")\n",
        "\n",
        "    # Determine config file to use\n",
        "    if SKIP_TRAINING:\n",
        "        eval_config = 'config/config_colab_full.yaml'  # Default for structure\n",
        "    else:\n",
        "        eval_config = TRAINING_CONFIG\n",
        "\n",
        "    print(f\"üìã Using config: {eval_config}\")\n",
        "    print(\"\\nüîç Evaluating model on test set...\")\n",
        "\n",
        "    # Run evaluation\n",
        "    !python src/evaluate.py --model_path {MODEL_PATH} --config {eval_config} --device cuda\n",
        "\n",
        "    print(\"\\n‚úÖ Evaluation completed!\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No trained model found. Please:\")\n",
        "    if SKIP_TRAINING:\n",
        "        print(\"   ‚Ä¢ Ensure you loaded a valid pre-trained model\")\n",
        "        print(\"   ‚Ä¢ Re-run the model loading cell if needed\")\n",
        "    else:\n",
        "        print(\"   ‚Ä¢ Complete training first\")\n",
        "        print(\"   ‚Ä¢ Check if training completed successfully\")\n",
        "\n",
        "# Set checkpoint files for other cells\n",
        "if MODEL_PATH:\n",
        "    checkpoint_files = [MODEL_PATH]\n",
        "    latest_checkpoint = MODEL_PATH\n",
        "else:\n",
        "    checkpoint_files = []\n",
        "    latest_checkpoint = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90575fb7",
      "metadata": {
        "id": "90575fb7"
      },
      "source": [
        "## 8. Interactive Text Generation\n",
        "\n",
        "Now let's have some fun! Generate text interactively with custom seed words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b35caa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3b35caa",
        "outputId": "51498f8a-19fc-4667-9d2d-9c50c4c14c65"
      },
      "outputs": [],
      "source": [
        "# Set up interactive text generation\n",
        "import sys\n",
        "sys.path.append('src')\n",
        "\n",
        "from model import create_model\n",
        "from data_loader import Vocabulary, load_ptb_data\n",
        "from utils import load_checkpoint, generate_text\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Load vocabulary using the proper data_loader approach\n",
        "print(\"üìñ Loading vocabulary using data_loader...\")\n",
        "vocab = Vocabulary()\n",
        "vocab_file = 'data/ptb/vocab.pkl'\n",
        "\n",
        "if os.path.exists(vocab_file):\n",
        "    vocab.load(vocab_file)\n",
        "    print(f\"üìñ Vocabulary loaded: {len(vocab)} words\")\n",
        "else:\n",
        "    print(\"‚ùå Vocabulary file not found. Please ensure training completed successfully.\")\n",
        "    vocab = None\n",
        "\n",
        "# Load model\n",
        "if MODEL_PATH and os.path.exists(MODEL_PATH) and vocab is not None:\n",
        "    print(f\"üîÑ Loading model: {os.path.basename(MODEL_PATH)}\")\n",
        "\n",
        "    # Detect the correct config from the model checkpoint\n",
        "    checkpoint_info = torch.load(MODEL_PATH, map_location='cpu')\n",
        "\n",
        "    # Check embedding dimension to determine which config was used\n",
        "    if 'model_state_dict' in checkpoint_info:\n",
        "        embedding_dim = checkpoint_info['model_state_dict']['embedding.weight'].shape[1]\n",
        "        vocab_size_from_model = checkpoint_info['model_state_dict']['embedding.weight'].shape[0]\n",
        "\n",
        "        print(f\"üîç Model vocabulary size: {vocab_size_from_model}\")\n",
        "        print(f\"üîç Loaded vocabulary size: {len(vocab)}\")\n",
        "\n",
        "        if vocab_size_from_model != len(vocab):\n",
        "            print(f\"‚ö†Ô∏è Vocabulary size mismatch!\")\n",
        "            print(f\"   Model expects: {vocab_size_from_model}\")\n",
        "            print(f\"   Current vocab: {len(vocab)}\")\n",
        "            print(\"   This may cause loading issues.\")\n",
        "\n",
        "        config_file = 'config/optimal_config.yaml'\n",
        "    else:\n",
        "        # Fallback to training config if available\n",
        "        if SKIP_TRAINING:\n",
        "            config_file = 'config/optimal_config.yaml'\n",
        "        else:\n",
        "            config_file = TRAINING_CONFIG\n",
        "\n",
        "    print(f\"üìã Using config: {config_file}\")\n",
        "\n",
        "    with open(config_file, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "\n",
        "    model = create_model(config, len(vocab))\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Load trained weights\n",
        "    model, _, _, _ = load_checkpoint(MODEL_PATH, model)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"‚úÖ Model loaded and ready for text generation!\")\n",
        "\n",
        "    def interactive_generate(seed_text=\"the\", max_length=50, temperature=1.0):\n",
        "        \"\"\"Generate text with custom parameters\"\"\"\n",
        "        print(f\"\\nüéØ Generating text...\")\n",
        "        print(f\"Seed: '{seed_text}' | Length: {max_length} | Temperature: {temperature}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        try:\n",
        "            generated = generate_text(\n",
        "                model=model,\n",
        "                vocab=vocab,\n",
        "                seed_text=seed_text,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            print(f\"üìù Generated text:\\n{generated}\")\n",
        "            return generated\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Generation failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Demonstrate different generation styles\n",
        "    print(\"üé® Text Generation Examples:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Conservative generation (low temperature)\n",
        "    print(\"1Ô∏è‚É£ Conservative (temperature=0.7):\")\n",
        "    interactive_generate(\"the economy\", max_length=40, temperature=0.7)\n",
        "\n",
        "    # Balanced generation\n",
        "    print(\"\\n2Ô∏è‚É£ Balanced (temperature=1.0):\")\n",
        "    interactive_generate(\"financial markets\", max_length=40, temperature=1.0)\n",
        "\n",
        "    # Creative generation (high temperature)\n",
        "    print(\"\\n3Ô∏è‚É£ Creative (temperature=1.3):\")\n",
        "    interactive_generate(\"investors\", max_length=40, temperature=1.3)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No trained model available.\")\n",
        "    if SKIP_TRAINING:\n",
        "        print(\"   ‚Ä¢ Please load a pre-trained model first\")\n",
        "        print(\"   ‚Ä¢ Re-run the model loading cell\")\n",
        "    else:\n",
        "        print(\"   ‚Ä¢ Please complete training first\")\n",
        "        print(\"   ‚Ä¢ Check if training completed successfully\")\n",
        "\n",
        "    # Create dummy function for error handling\n",
        "    def interactive_generate(seed_text=\"the\", max_length=50, temperature=1.0):\n",
        "        print(\"‚ùå No model available for text generation\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f36b0e80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f36b0e80",
        "outputId": "34ff44d8-9c78-4698-9889-72891de5751f"
      },
      "outputs": [],
      "source": [
        "# Interactive text generation widget\n",
        "print(\"üéÆ Interactive Text Generation\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
        "    print(\"Customize your text generation below:\")\n",
        "\n",
        "    # Simple interactive interface\n",
        "    def generate_custom_text():\n",
        "        \"\"\"Interactive text generation function\"\"\"\n",
        "\n",
        "        print(\"\\nüìù Enter your parameters:\")\n",
        "\n",
        "        # Get user inputs\n",
        "        seed_text = input(\"Seed text (e.g., 'the market'): \").strip() or \"the\"\n",
        "\n",
        "        try:\n",
        "            max_length = int(input(\"Maximum length (10-100): \") or \"30\")\n",
        "            max_length = max(10, min(100, max_length))  # Clamp between 10-100\n",
        "        except ValueError:\n",
        "            max_length = 30\n",
        "            print(f\"Using default length: {max_length}\")\n",
        "\n",
        "        try:\n",
        "            temperature = float(input(\"Temperature (0.5-2.0, higher=more creative): \") or \"1.0\")\n",
        "            temperature = max(0.1, min(2.0, temperature))  # Clamp between 0.1-2.0\n",
        "        except ValueError:\n",
        "            temperature = 1.0\n",
        "            print(f\"Using default temperature: {temperature}\")\n",
        "\n",
        "        # Generate text\n",
        "        generated = interactive_generate(seed_text, max_length, temperature)\n",
        "\n",
        "        if generated:\n",
        "            print(f\"\\n‚ú® Want to try again? Call generate_custom_text() or try different parameters!\")\n",
        "\n",
        "        return generated\n",
        "\n",
        "    # Instructions for use\n",
        "    print(\"\\nüí° Instructions:\")\n",
        "    print(\"‚Ä¢ Call generate_custom_text() to start interactive generation\")\n",
        "    print(\"‚Ä¢ Or use interactive_generate('your seed', length, temperature) directly\")\n",
        "    print(\"‚Ä¢ Examples:\")\n",
        "    print(\"  - interactive_generate('the stock market', 50, 1.0)\")\n",
        "    print(\"  - interactive_generate('investors', 30, 0.8)\")\n",
        "\n",
        "    # Ready message\n",
        "    print(f\"\\nüéØ Model ready: {os.path.basename(MODEL_PATH)}\")\n",
        "    print(\"Ready for text generation! üöÄ\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No model available for interactive generation.\")\n",
        "    if SKIP_TRAINING:\n",
        "        print(\"   ‚Ä¢ Please load a pre-trained model first\")\n",
        "    else:\n",
        "        print(\"   ‚Ä¢ Please complete training first\")\n",
        "\n",
        "    def generate_custom_text():\n",
        "        print(\"‚ùå No model available. Please load a model first.\")\n",
        "        return None\n",
        "\n",
        "# Instructions for users\n",
        "print(\"\\nüí° Tips for text generation:\")\n",
        "print(\"‚Ä¢ Lower temperature (0.5-0.8): More predictable, grammatical text\")\n",
        "print(\"‚Ä¢ Higher temperature (1.2-2.0): More creative, diverse text\")\n",
        "print(\"‚Ä¢ Seed text: Use Penn Treebank vocabulary for best results\")\n",
        "print(\"‚Ä¢ Common seeds: 'the', 'market', 'company', 'economic', 'financial'\")\n",
        "\n",
        "print(\"\\nüëá Run the cell below to start interactive generation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f7915f44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "f7915f44",
        "outputId": "c302c926-9266-4b30-f4fd-4a02c78ee3d0"
      },
      "outputs": [],
      "source": [
        "# Run interactive generation\n",
        "generate_custom_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "gIUJ7qajuPkW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIUJ7qajuPkW",
        "outputId": "cc72dfda-4ac1-491e-fa73-d589b0d6b942"
      },
      "outputs": [],
      "source": [
        "# Show most common words in vocabulary\n",
        "print(\"üîù Most common words in Penn Treebank:\")\n",
        "# Get first 20 most frequent words\n",
        "sample_words = list(vocab.word2idx.keys())[:20]\n",
        "for i, word in enumerate(sample_words, 1):\n",
        "    print(f\"{i:2d}. {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11a53b0",
      "metadata": {
        "id": "e11a53b0"
      },
      "source": [
        "## 9. Results and Conclusion\n",
        "\n",
        "Summary of training results and model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a5be77",
      "metadata": {
        "id": "06a5be77"
      },
      "outputs": [],
      "source": [
        "# Summarize results\n",
        "print(\"üìä Training Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if checkpoint_files:\n",
        "    # Get checkpoint info\n",
        "    checkpoint_info = torch.load(latest_checkpoint, map_location='cpu')\n",
        "\n",
        "    print(f\"‚úÖ Model trained successfully!\")\n",
        "    print(f\"üìÅ Best model: {os.path.basename(latest_checkpoint)}\")\n",
        "    print(f\"üî¢ Final epoch: {checkpoint_info.get('epoch', 'Unknown')}\")\n",
        "    print(f\"üìâ Final loss: {checkpoint_info.get('loss', 'Unknown'):.4f}\")\n",
        "\n",
        "    # Model architecture summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
        "    print(f\"‚Ä¢ Total parameters: {total_params:,}\")\n",
        "    print(f\"‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"‚Ä¢ Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "    print(f\"\\nüéØ Key Results:\")\n",
        "    print(f\"‚Ä¢ Successfully trained LSTM language model\")\n",
        "    print(f\"‚Ä¢ Model can generate coherent text sequences\")\n",
        "    print(f\"‚Ä¢ Interactive generation with customizable parameters\")\n",
        "    print(f\"‚Ä¢ Ready for deployment or further fine-tuning\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trained model found.\")\n",
        "\n",
        "print(f\"\\nüìÅ Generated Files:\")\n",
        "if os.path.exists('checkpoints') and os.listdir('checkpoints'):\n",
        "    print(f\"‚Ä¢ Checkpoints: {len(os.listdir('checkpoints'))} files\")\n",
        "if os.path.exists('runs') and os.listdir('runs'):\n",
        "    print(f\"‚Ä¢ TensorBoard logs: Available in runs/\")\n",
        "\n",
        "print(f\"\\nüéâ Training and evaluation completed!\")\n",
        "print(f\"You can now experiment with different seed texts and parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f282905c",
      "metadata": {
        "id": "f282905c"
      },
      "source": [
        "## 10. Download Results (Optional)\n",
        "\n",
        "Package and download your training results for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8107ea0",
      "metadata": {
        "id": "e8107ea0"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "def package_results():\n",
        "    \"\"\"Package all generated files for download\"\"\"\n",
        "\n",
        "    print(\"üì¶ Packaging results...\")\n",
        "\n",
        "    items_to_include = []\n",
        "\n",
        "    # Check what we have\n",
        "    if os.path.exists('checkpoints') and os.listdir('checkpoints'):\n",
        "        checkpoint_count = len([f for f in os.listdir('checkpoints') if f.endswith(('.pth', '.pt'))])\n",
        "        if checkpoint_count > 0:\n",
        "            items_to_include.append(('checkpoints', f'Model checkpoints ({checkpoint_count} files)'))\n",
        "\n",
        "    if os.path.exists('runs') and os.listdir('runs'):\n",
        "        items_to_include.append(('runs', 'TensorBoard logs'))\n",
        "\n",
        "    # Include vocabulary and config files\n",
        "    if os.path.exists('data/ptb/vocab.pkl'):\n",
        "        items_to_include.append(('data/ptb/vocab.pkl', 'Vocabulary file'))\n",
        "\n",
        "    # Include relevant config file\n",
        "    if not SKIP_TRAINING and TRAINING_CONFIG and os.path.exists(TRAINING_CONFIG):\n",
        "        items_to_include.append((TRAINING_CONFIG, 'Training configuration'))\n",
        "\n",
        "    if not items_to_include:\n",
        "        print(\"‚ö†Ô∏è No results to package.\")\n",
        "        if SKIP_TRAINING:\n",
        "            print(\"   ‚Ä¢ Pre-trained model mode - no training artifacts to package\")\n",
        "        else:\n",
        "            print(\"   ‚Ä¢ Run training first to generate results\")\n",
        "        return None\n",
        "\n",
        "    # Create results archive\n",
        "    zip_filename = f'penn_treebank_results_{TRAINING_MODE}.zip'\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for item_path, description in items_to_include:\n",
        "            print(f\"   üìÅ Adding {description}...\")\n",
        "\n",
        "            if os.path.isfile(item_path):\n",
        "                # Single file\n",
        "                zipf.write(item_path, os.path.basename(item_path))\n",
        "            else:\n",
        "                # Directory\n",
        "                for root, dirs, files in os.walk(item_path):\n",
        "                    for file in files:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        arcname = os.path.relpath(file_path, '.')\n",
        "                        zipf.write(file_path, arcname)\n",
        "\n",
        "    print(f\"‚úÖ Results packaged in: {zip_filename}\")\n",
        "    return zip_filename\n",
        "\n",
        "# Package and download\n",
        "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
        "    zip_file = package_results()\n",
        "\n",
        "    if zip_file:\n",
        "        print(\"\\n‚¨áÔ∏è Downloading results...\")\n",
        "        try:\n",
        "            files.download(zip_file)\n",
        "            print(\"‚úÖ Download started! Check your browser's download folder.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Download failed: {e}\")\n",
        "            print(f\"   ‚Ä¢ File saved locally as: {zip_file}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No results to download.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No training results to download.\")\n",
        "    if SKIP_TRAINING:\n",
        "        print(\"   ‚Ä¢ Pre-trained model mode - no training artifacts to download\")\n",
        "    else:\n",
        "        print(\"   ‚Ä¢ Complete training first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290560b0",
      "metadata": {
        "id": "290560b0"
      },
      "source": [
        "---\n",
        "\n",
        "## üéØ Next Steps\n",
        "\n",
        "**Experiment Further:**\n",
        "- Try different model architectures (GRU, Transformer)\n",
        "- Experiment with hyperparameters in config files\n",
        "- Train on different datasets\n",
        "- Implement beam search for better text generation\n",
        "\n",
        "**Deploy Your Model:**\n",
        "- Export model for production use\n",
        "- Create a web interface for text generation\n",
        "- Fine-tune on domain-specific text\n",
        "\n",
        "**Learn More:**\n",
        "- Study attention mechanisms\n",
        "- Explore modern transformer architectures\n",
        "- Experiment with different text generation techniques\n",
        "\n",
        "---\n",
        "\n",
        "*This notebook demonstrated complete language modeling pipeline from data exploration to interactive text generation. Feel free to modify and extend it for your own experiments!*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
