{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c671ef",
   "metadata": {},
   "source": [
    "# Penn Treebank Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f11ce",
   "metadata": {
    "id": "3a1f11ce"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook enables training a language model on the Penn Treebank dataset using GPU acceleration in Google Colab.\n",
    "\n",
    "**Features:**\n",
    "- Automatic environment setup and dependency installation\n",
    "- **Runtime data extraction** to minimize Google Drive upload time\n",
    "- GPU verification and optimization\n",
    "- Two training configurations: quick test and full training\n",
    "- Automatic model checkpointing to Google Drive\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro (recommended for longer training sessions)\n",
    "- Project folder uploaded to Google Drive (with compressed data only)\n",
    "- GPU runtime enabled in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5880a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56b5880a",
    "outputId": "411453a1-f8d1-422f-aaa1-5e669aba444a"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify mount was successful\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "    print(f\"Drive contents: {os.listdir('/content/drive/MyDrive')[:5]}...\")  # Show first 5 items\n",
    "else:\n",
    "    print(\"‚ùå Failed to mount Google Drive. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1c1b7",
   "metadata": {
    "id": "60d1c1b7"
   },
   "source": [
    "## üîß Setup Project Directory\n",
    "\n",
    "**Instructions:**\n",
    "1. Locate your `Proj-2-Penn Treebank (PTB)` folder in Google Drive\n",
    "2. Update the path in the next cell\n",
    "3. **Important:** Remove the extracted `LDC99T42/` folder from your project before uploading to save time - we'll extract it at runtime!\n",
    "\n",
    "**Common paths:**\n",
    "- If uploaded to root: `/content/drive/MyDrive/Proj-2-Penn Treebank (PTB)`\n",
    "- If in a subfolder: `/content/drive/MyDrive/YourFolder/Proj-2-Penn Treebank (PTB)`\n",
    "- If in Colab Notebooks folder: `/content/drive/MyDrive/Colab Notebooks/Proj-2-Penn Treebank (PTB)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f4328",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae1f4328",
    "outputId": "23024c01-4906-4e90-8ea4-f2a77092ee5d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_path = '/content/drive/MyDrive/Proj-2-Penn Treebank (PTB)'\n",
    "\n",
    "# Verify the project directory exists\n",
    "if not os.path.exists(project_path):\n",
    "    print(f\"‚ùå Project directory not found: {project_path}\")\n",
    "    print(\"\\nüîç Searching for the project folder...\")\n",
    "\n",
    "    # Search for the project folder\n",
    "    drive_root = '/content/drive/MyDrive'\n",
    "    for root, dirs, files in os.walk(drive_root):\n",
    "        if 'Proj-2-Penn Treebank (PTB)' in dirs:\n",
    "            suggested_path = os.path.join(root, 'Proj-2-Penn Treebank (PTB)')\n",
    "            print(f\"Found project at: {suggested_path}\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nPlease update the 'project_path' variable above with the correct path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(project_path)\n",
    "print(f\"‚úÖ Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify project structure - now only requiring compressed archive\n",
    "required_items = ['src', 'config', 'data', 'requirements.txt']\n",
    "required_data_files = ['data/ptb/LDC99T42_Penn_Treebank_3.tar.zst']\n",
    "\n",
    "missing_items = [item for item in required_items if not os.path.exists(item)]\n",
    "missing_data = [item for item in required_data_files if not os.path.exists(item)]\n",
    "\n",
    "if missing_items:\n",
    "    print(f\"‚ö†Ô∏è Missing required directories: {missing_items}\")\n",
    "if missing_data:\n",
    "    print(f\"‚ö†Ô∏è Missing required data files: {missing_data}\")\n",
    "    print(\"   Make sure LDC99T42_Penn_Treebank_3.tar.zst is in the data/ptb/ directory\")\n",
    "\n",
    "if not missing_items and not missing_data:\n",
    "    print(\"‚úÖ Project structure verified!\")\n",
    "    print(f\"Project contents: {os.listdir('.')[:10]}...\")  # Show first 10 items\n",
    "\n",
    "    # Check if data is already extracted\n",
    "    if os.path.exists('data/ptb/LDC99T42'):\n",
    "        print(\"üìÅ Data already extracted\")\n",
    "    else:\n",
    "        print(\"üì¶ Data needs to be extracted (will be done in next step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b43d9",
   "metadata": {
    "id": "f52b43d9"
   },
   "source": [
    "## üì¶ Extract Penn Treebank Data\n",
    "\n",
    "This step extracts the compressed Penn Treebank data at runtime to avoid uploading large extracted files to Google Drive. This significantly reduces upload time while maintaining full functionality.\n",
    "\n",
    "**Process:**\n",
    "1. Extract `LDC99T42_Penn_Treebank_3.tar.zst` to temporary local storage\n",
    "2. Process raw data files to create train/valid/test splits\n",
    "3. Keep only the processed text files (much smaller)\n",
    "\n",
    "**Benefits:**\n",
    "- ‚ö° Faster Google Drive uploads (no large extracted data)\n",
    "- üíæ Efficient storage usage\n",
    "- üîÑ Fresh extraction each session ensures data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da69d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3da69d2",
    "outputId": "abc95058-1060-4c76-f44f-6aab3056509b"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if data extraction is needed\n",
    "data_dir = Path('data/ptb')\n",
    "archive_path = data_dir / 'LDC99T42_Penn_Treebank_3.tar.zst'\n",
    "extracted_dir = data_dir / 'LDC99T42'\n",
    "processed_files = ['ptb.train.txt', 'ptb.valid.txt', 'ptb.test.txt']\n",
    "\n",
    "# Check if processed files already exist\n",
    "all_processed_exist = all((data_dir / f).exists() for f in processed_files)\n",
    "\n",
    "if all_processed_exist:\n",
    "    print(\"‚úÖ Processed Penn Treebank data already exists\")\n",
    "    for f in processed_files:\n",
    "        file_path = data_dir / f\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   üìÑ {f}: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(\"üöÄ Extracting and processing Penn Treebank data...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check if archive exists\n",
    "    if not archive_path.exists():\n",
    "        print(f\"‚ùå Archive not found: {archive_path}\")\n",
    "        print(\"Please ensure LDC99T42_Penn_Treebank_3.tar.zst is in the data/ptb/ directory\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"üì¶ Found archive: {archive_path.name} ({archive_path.stat().st_size / (1024*1024):.1f} MB)\")\n",
    "\n",
    "    # Install zstd if not available (for .zst decompression)\n",
    "    try:\n",
    "        subprocess.run(['zstd', '--version'], capture_output=True, check=True)\n",
    "        print(\"‚úÖ zstd already installed\")\n",
    "    except:\n",
    "        print(\"üì• Installing zstd for archive extraction...\")\n",
    "        !apt-get update -qq && apt-get install -y zstd\n",
    "\n",
    "    # Extract the archive\n",
    "    print(\"üîÑ Extracting archive...\")\n",
    "    try:\n",
    "        # First decompress .zst to .tar\n",
    "        tar_path = archive_path.with_suffix('')  # Remove .zst extension\n",
    "        subprocess.run(['zstd', '-d', str(archive_path), '-o', str(tar_path)], check=True)\n",
    "\n",
    "        # Then extract the tar file\n",
    "        with tarfile.open(tar_path, 'r') as tar:\n",
    "            tar.extractall(path=data_dir)\n",
    "\n",
    "        # Clean up the intermediate .tar file\n",
    "        tar_path.unlink()\n",
    "\n",
    "        print(f\"‚úÖ Archive extracted to: {extracted_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extraction failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Process the extracted data using the preprocessing script\n",
    "    print(\"üîÑ Processing raw data into train/valid/test splits...\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            'python', 'scripts/preprocess_ptb.py',\n",
    "            '--ptb_root', str(extracted_dir),\n",
    "            '--output_dir', str(data_dir),\n",
    "            '--verify'\n",
    "        ], check=True)\n",
    "\n",
    "        print(\"‚úÖ Data processing completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data processing failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Clean up extracted directory to save space (keep only processed files)\n",
    "    if extracted_dir.exists():\n",
    "        print(\"üßπ Cleaning up extracted files to save space...\")\n",
    "        shutil.rmtree(extracted_dir)\n",
    "        print(\"‚úÖ Cleanup completed\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n‚è±Ô∏è Data extraction and processing completed in {end_time - start_time:.1f} seconds\")\n",
    "\n",
    "    # Show final processed files\n",
    "    print(\"\\nüìä Final processed data files:\")\n",
    "    for f in processed_files:\n",
    "        file_path = data_dir / f\n",
    "        if file_path.exists():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   üìÑ {f}: {size_mb:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {f}: Missing\")\n",
    "\n",
    "print(\"\\nüéâ Penn Treebank data is ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71033c0b",
   "metadata": {
    "id": "71033c0b"
   },
   "source": [
    "## üöÄ Environment Setup\n",
    "\n",
    "### GPU Verification\n",
    "First, let's verify that GPU is available and properly configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52542da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"GPU Available: \", torch.cuda.is_available())\n",
    "print(\"GPU: \", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb6678",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bbb6678",
    "outputId": "82f75245-be12-4305-de7b-eb1ffed9bc59"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"üîç GPU Information:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected. Make sure to enable GPU runtime in Colab!\")\n",
    "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "\n",
    "# Check nvidia-smi for additional info\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    print(\"\\nüìä GPU Status:\")\n",
    "    print(result.stdout)\n",
    "except:\n",
    "    print(\"Could not run nvidia-smi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ca362",
   "metadata": {
    "id": "573ca362"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3422c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15b3422c",
    "outputId": "61edc30d-b93f-45c1-f407-0a3e87dac584"
   },
   "outputs": [],
   "source": [
    "# Install project dependencies\n",
    "print(\"üì¶ Installing dependencies from requirements.txt...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Verify key installations\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n‚úÖ Key packages verified:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d3d95",
   "metadata": {
    "id": "d13d3d95"
   },
   "source": [
    "## üß™ Quick Test Run\n",
    "\n",
    "This test will validate your environment before running the full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2faf9",
   "metadata": {
    "id": "43d2faf9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# Pre-run validation\n",
    "print(\"üîç Pre-run validation:\")\n",
    "required_files = [\n",
    "    'src/train.py',\n",
    "    'config/config_colab_quick.yaml',\n",
    "    'data/ptb/ptb.train.txt'\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} - Missing!\")\n",
    "\n",
    "print(\"\\nüöÄ Starting quick test training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run training with enhanced output\n",
    "!python src/train.py --config config/config_colab_quick.yaml --device cuda\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n‚è±Ô∏è Quick test completed in {end_time - start_time:.1f} seconds\")\n",
    "print(f\"üìÅ Check the 'checkpoints' directory for saved models\")\n",
    "print(f\"üìä TensorBoard logs saved in 'runs' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mW8qt-u7LxTG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mW8qt-u7LxTG",
    "outputId": "ffeb92fd-f6b5-47c2-ed48-e210741ce996"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Starting full training run...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run training with enhanced output\n",
    "!python src/train.py --config config/config_colab_full.yaml --device cuda\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n‚è±Ô∏è Full training completed in {end_time - start_time:.1f} seconds\")\n",
    "print(f\"üìÅ Check the 'checkpoints' directory for saved models\")\n",
    "print(f\"üìä TensorBoard logs saved in 'runs' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b3e78",
   "metadata": {
    "id": "c60b3e78"
   },
   "source": [
    "### Model Evaluation\n",
    "Evaluate your trained model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ecf09d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4ecf09d",
    "outputId": "b046fa0c-90ab-4c6e-90ed-8b1ba3b0bc11"
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "import glob\n",
    "\n",
    "# Find the latest checkpoint\n",
    "checkpoint_files = glob.glob('checkpoints_full/*.pth')\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "    print(f\"üìÅ Latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "    # Run evaluation\n",
    "    print(\"\\nüßÆ Evaluating model on test set...\")\n",
    "    !python src/evaluate.py --checkpoint {latest_checkpoint} --config config/config_colab_full.yaml --device cuda\n",
    "else:\n",
    "    print(\"‚ùå No checkpoints found. Make sure training completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
