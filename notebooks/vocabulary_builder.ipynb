{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3974cf2",
   "metadata": {},
   "source": [
    "# Penn Treebank Vocabulary Builder\n",
    "\n",
    "This notebook builds the vocabulary for the Penn Treebank dataset based on the analysis recommendations:\n",
    "- **Recommended vocab size**: 30,000 words (frequency â‰¥ 3)\n",
    "- **Coverage**: 99.1% of training tokens\n",
    "- **Special tokens**: `<pad>`, `<unk>`, `<eos>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "from data_loader import Vocabulary, load_ptb_data\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths\n",
    "data_dir = '../data/ptb'\n",
    "vocab_file = os.path.join(data_dir, 'vocab.pkl')\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Vocabulary file: {vocab_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c2555",
   "metadata": {},
   "source": [
    "## Step 1: Load Raw Text Data\n",
    "\n",
    "First, let's load the raw Penn Treebank text files and examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c85e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw text files\n",
    "train_file = os.path.join(data_dir, 'ptb.train.txt')\n",
    "valid_file = os.path.join(data_dir, 'ptb.valid.txt')\n",
    "test_file = os.path.join(data_dir, 'ptb.test.txt')\n",
    "\n",
    "# Check if files exist\n",
    "for file_path, split_name in [(train_file, 'train'), (valid_file, 'valid'), (test_file, 'test')]:\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            lines = content.split('\\n')\n",
    "            words = content.split()\n",
    "            print(f\"{split_name.upper()}: {len(lines):,} sentences, {len(words):,} words\")\n",
    "    else:\n",
    "        print(f\"{split_name.upper()}: File not found at {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce6f70",
   "metadata": {},
   "source": [
    "## Step 2: Build Word Frequency Distribution\n",
    "\n",
    "Let's analyze the word frequency distribution to determine optimal vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f405ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training text\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read().strip()\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter()\n",
    "for line in train_text.split('\\n'):\n",
    "    if line.strip():\n",
    "        words = line.strip().split()\n",
    "        for word in words:\n",
    "            word_counts[word] += 1\n",
    "\n",
    "print(f\"Total unique words in training set: {len(word_counts):,}\")\n",
    "print(f\"Total word tokens in training set: {sum(word_counts.values()):,}\")\n",
    "\n",
    "# Show most common words\n",
    "print(\"\\nTop 20 most frequent words:\")\n",
    "for word, count in word_counts.most_common(20):\n",
    "    print(f\"  {word}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze frequency distribution\n",
    "frequencies = list(word_counts.values())\n",
    "frequencies.sort(reverse=True)\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Full distribution (log scale)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(frequencies)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Word Rank')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.title('Word Frequency Distribution')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Cumulative coverage\n",
    "cumulative_freq = np.cumsum(frequencies)\n",
    "coverage = cumulative_freq / cumulative_freq[-1]\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(coverage[:10000])  # First 10K words\n",
    "plt.xlabel('Vocabulary Size')\n",
    "plt.ylabel('Token Coverage')\n",
    "plt.title('Cumulative Token Coverage')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Different minimum frequency thresholds\n",
    "min_freqs = [1, 2, 3, 4, 5, 10, 20]\n",
    "vocab_sizes = []\n",
    "coverages = []\n",
    "\n",
    "for min_freq in min_freqs:\n",
    "    vocab_size = sum(1 for count in word_counts.values() if count >= min_freq)\n",
    "    covered_tokens = sum(count for count in word_counts.values() if count >= min_freq)\n",
    "    coverage = covered_tokens / sum(word_counts.values())\n",
    "    vocab_sizes.append(vocab_size)\n",
    "    coverages.append(coverage)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "for i, min_freq in enumerate(min_freqs):\n",
    "    plt.scatter(vocab_sizes[i], coverages[i] * 100, s=60, \n",
    "               label=f'min_freq={min_freq}')\n",
    "\n",
    "plt.xlabel('Vocabulary Size')\n",
    "plt.ylabel('Coverage (%)')\n",
    "plt.title('Vocabulary Size vs Coverage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics for different thresholds\n",
    "print(\"\\nVocabulary size and coverage for different minimum frequencies:\")\n",
    "print(\"Min Freq | Vocab Size | Coverage\")\n",
    "print(\"-\" * 35)\n",
    "for i, min_freq in enumerate(min_freqs):\n",
    "    print(f\"{min_freq:8d} | {vocab_sizes[i]:10,d} | {coverages[i]*100:7.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5187e5",
   "metadata": {},
   "source": [
    "## Step 3: Build Vocabulary with Optimal Parameters\n",
    "\n",
    "Based on the analysis, we'll build the vocabulary with minimum frequency = 3 to get ~30K words as recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62318be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing vocabulary if it exists\n",
    "if os.path.exists(vocab_file):\n",
    "    os.remove(vocab_file)\n",
    "    print(f\"Removed existing vocabulary file: {vocab_file}\")\n",
    "\n",
    "# Build vocabulary with min_freq=3 (recommended setting)\n",
    "MIN_FREQ = 3\n",
    "\n",
    "print(f\"Building vocabulary with min_freq={MIN_FREQ}...\")\n",
    "\n",
    "# Create vocabulary object\n",
    "vocab = Vocabulary(special_tokens=['<pad>', '<unk>', '<eos>'])\n",
    "\n",
    "# Build vocabulary from training text\n",
    "vocab.build_vocab([train_text], min_freq=MIN_FREQ)\n",
    "\n",
    "print(f\"Vocabulary built successfully!\")\n",
    "print(f\"Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"Word count entries: {len(vocab.word_count):,}\")\n",
    "\n",
    "# Save vocabulary\n",
    "vocab.save(vocab_file)\n",
    "print(f\"Vocabulary saved to: {vocab_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4988fc",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Built Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the built vocabulary\n",
    "print(\"Vocabulary Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total vocabulary size: {len(vocab):,}\")\n",
    "print(f\"Special tokens: {vocab.special_tokens}\")\n",
    "\n",
    "# Check special token indices\n",
    "for token in vocab.special_tokens:\n",
    "    if token in vocab.word2idx:\n",
    "        print(f\"  {token}: index {vocab.word2idx[token]}\")\n",
    "\n",
    "# Calculate coverage on training set\n",
    "covered_tokens = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for word, count in vocab.word_count.items():\n",
    "    total_tokens += count\n",
    "    if word in vocab.word2idx:\n",
    "        covered_tokens += count\n",
    "\n",
    "coverage = covered_tokens / total_tokens\n",
    "print(f\"\\nTraining set coverage: {coverage*100:.2f}%\")\n",
    "\n",
    "# Show vocabulary statistics\n",
    "vocab_word_counts = [vocab.word_count[word] for word in vocab.word2idx if word not in vocab.special_tokens]\n",
    "print(f\"Average word frequency in vocab: {np.mean(vocab_word_counts):.2f}\")\n",
    "print(f\"Median word frequency in vocab: {np.median(vocab_word_counts):.2f}\")\n",
    "print(f\"Min word frequency in vocab: {min(vocab_word_counts)}\")\n",
    "print(f\"Max word frequency in vocab: {max(vocab_word_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77df132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vocabulary encoding/decoding\n",
    "print(\"Testing vocabulary encoding/decoding:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with sample sentences\n",
    "test_sentences = [\n",
    "    \"the quick brown fox jumps over the lazy dog <eos>\",\n",
    "    \"natural language processing is fascinating <eos>\",\n",
    "    \"out-of-vocabulary words should become <unk> tokens <eos>\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(f\"Original: {sentence}\")\n",
    "    \n",
    "    # Encode\n",
    "    encoded = vocab.encode(sentence)\n",
    "    print(f\"Encoded:  {encoded[:10]}...\" if len(encoded) > 10 else f\"Encoded:  {encoded}\")\n",
    "    \n",
    "    # Decode\n",
    "    decoded = vocab.decode(encoded)\n",
    "    print(f\"Decoded:  {decoded}\")\n",
    "    \n",
    "    # Check for unknown words\n",
    "    unknown_count = encoded.count(vocab.word2idx['<unk>'])\n",
    "    print(f\"Unknown words: {unknown_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6cf40",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate on Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7787dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vocabulary coverage on validation and test sets\n",
    "def evaluate_coverage(text_file, vocab, split_name):\n",
    "    \"\"\"Evaluate vocabulary coverage on a text file.\"\"\"\n",
    "    with open(text_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    \n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Count known and unknown words\n",
    "    known_words = 0\n",
    "    unknown_words = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocab.word2idx:\n",
    "            known_words += 1\n",
    "        else:\n",
    "            unknown_words += 1\n",
    "    \n",
    "    coverage = known_words / total_words if total_words > 0 else 0\n",
    "    oov_rate = unknown_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    print(f\"{split_name.upper()} Set Evaluation:\")\n",
    "    print(f\"  Total words: {total_words:,}\")\n",
    "    print(f\"  Known words: {known_words:,}\")\n",
    "    print(f\"  Unknown words: {unknown_words:,}\")\n",
    "    print(f\"  Coverage: {coverage*100:.2f}%\")\n",
    "    print(f\"  OOV rate: {oov_rate*100:.2f}%\")\n",
    "    \n",
    "    return coverage, oov_rate\n",
    "\n",
    "# Evaluate on all splits\n",
    "print(\"Vocabulary Coverage Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training set (should be very high since vocab was built from it)\n",
    "train_coverage, train_oov = evaluate_coverage(train_file, vocab, 'train')\n",
    "print()\n",
    "\n",
    "# Validation set\n",
    "valid_coverage, valid_oov = evaluate_coverage(valid_file, vocab, 'valid')\n",
    "print()\n",
    "\n",
    "# Test set\n",
    "test_coverage, test_oov = evaluate_coverage(test_file, vocab, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f061a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Coverage comparison\n",
    "splits = ['Train', 'Valid', 'Test']\n",
    "coverages = [train_coverage*100, valid_coverage*100, test_coverage*100]\n",
    "oov_rates = [train_oov*100, valid_oov*100, test_oov*100]\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(splits, coverages, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax1.set_ylabel('Coverage (%)')\n",
    "ax1.set_title('Vocabulary Coverage by Split')\n",
    "ax1.set_ylim(90, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, coverage in zip(bars1, coverages):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{coverage:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# OOV rate comparison\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(splits, oov_rates, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax2.set_ylabel('OOV Rate (%)')\n",
    "ax2.set_title('Out-of-Vocabulary Rate by Split')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, oov in zip(bars2, oov_rates):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "             f'{oov:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4521f",
   "metadata": {},
   "source": [
    "## Step 6: Final Summary\n",
    "\n",
    "The vocabulary has been successfully built and saved. Here's a summary of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892945dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"VOCABULARY BUILDING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Vocabulary file: {vocab_file}\")\n",
    "print(f\"Vocabulary size: {len(vocab):,} words\")\n",
    "print(f\"Minimum frequency threshold: {MIN_FREQ}\")\n",
    "print()\n",
    "print(\"Coverage Results:\")\n",
    "print(f\"  Training:   {train_coverage*100:.2f}% coverage, {train_oov*100:.2f}% OOV\")\n",
    "print(f\"  Validation: {valid_coverage*100:.2f}% coverage, {valid_oov*100:.2f}% OOV\") \n",
    "print(f\"  Test:       {test_coverage*100:.2f}% coverage, {test_oov*100:.2f}% OOV\")\n",
    "print()\n",
    "print(\"Special tokens:\")\n",
    "for token in vocab.special_tokens:\n",
    "    if token in vocab.word2idx:\n",
    "        print(f\"  {token}: index {vocab.word2idx[token]}\")\n",
    "\n",
    "print()\n",
    "print(\"The vocabulary is ready for training language models!\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Load vocabulary using: vocab = Vocabulary(); vocab.load('vocab.pkl')\")\n",
    "print(\"2. Use with data loaders for training\")\n",
    "print(\"3. Begin model training with LSTM or Transformer architectures\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
